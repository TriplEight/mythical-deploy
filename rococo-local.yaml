apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-17T05:50:45Z"
    generateName: chainspec-85d9949cc8-
    labels:
      pod-template-hash: 85d9949cc8
      run: chainspec
    name: chainspec-85d9949cc8-d5d9r
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: chainspec-85d9949cc8
      uid: 2694a96b-93b2-479d-a9b1-644db1559b30
    resourceVersion: "35636719"
    uid: ad7885d1-654f-4133-9bba-d1b97680978b
  spec:
    containers:
    - image: nginx:stable
      imagePullPolicy: IfNotPresent
      name: chainspec
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bqd8z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - -c
      - polkadot build-spec --chain rococo-local --disable-default-bootnode --raw  >
        /dir/rococo-local.json
      command:
      - /bin/sh
      image: ddorgan/tellor-polkadot:dev
      imagePullPolicy: IfNotPresent
      name: create-chainspec-rococolocal
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dir
        name: dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bqd8z
        readOnly: true
    - args:
      - -c
      - |
        set -xe; polkadot-parachain build-spec --chain statemint-local --disable-default-bootnode > /dir/statemint-plain.json; sed 's/"relay_chain": "polkadot-local"/"relay_chain": "rococo_local_testnet"/' -i /dir/statemint-plain.json; polkadot-parachain build-spec --chain /dir/statemint-plain.json --disable-default-bootnode --raw  > /dir/statemint.json;
      command:
      - /bin/sh
      image: ddorgan/polkadot-parachain:dev
      imagePullPolicy: IfNotPresent
      name: create-chainspec-statemint
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dir
        name: dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bqd8z
        readOnly: true
    - args:
      - -c
      - |
        set -xe; moonbeam build-spec --chain moonbase-local --disable-default-bootnode > /dir/moonbase-dev-plain.json; sed 's/"relayChain": "westend-local"/"relayChain": "rococo_local_testnet"/' -i /dir/moonbase-dev-plain.json; sed 's/"paraId": 1000/"paraId": 2000/' -i /dir/moonbase-dev-plain.json; sed 's/"parachainId": 1000/"parachainId": 2000/' -i /dir/moonbase-dev-plain.json; moonbeam build-spec --chain /dir/moonbase-dev-plain.json --disable-default-bootnode --raw > /dir/moonbase.json;
      command:
      - /bin/sh
      image: ddorgan/moonbeam:dev
      imagePullPolicy: IfNotPresent
      name: create-chainspec-moonbase
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dir
        name: dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bqd8z
        readOnly: true
    - args:
      - -c
      - |
        set -xe; parachain-template-node build-spec  --disable-default-bootnode > /dir/tellor-plain.json; sed 's/"para_id": 1000/"para_id": 3000/' -i /dir/tellor-plain.json; sed 's/"relay_chain": "rococo-local"/"relay_chain": "rococo_local_testnet"/' -i /dir/tellor-plain.json; sed 's/"parachainId": 1000/"parachainId": 3000/' -i /dir/tellor-plain.json; parachain-template-node build-spec --chain /dir/tellor-plain.json --disable-default-bootnode --raw > /dir/tellor.json;
      command:
      - /bin/sh
      image: ddorgan/tellor-oracle:rust-locked
      imagePullPolicy: IfNotPresent
      name: create-chainspec-tellor
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dir
        name: dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bqd8z
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-f7dd
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: dir
    - name: kube-api-access-bqd8z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:51:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:51:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:51:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:50:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://34ec77c7762c6b74893cb6b81db27fcf8b1474ca91382180ffaaebb4e640cfc1
      image: docker.io/library/nginx:stable
      imageID: docker.io/library/nginx@sha256:b1a2c7bcc61be621eae24851a976179bfbc72591e43c1fb340f7497ff72128ff
      lastState: {}
      name: chainspec
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-05-17T05:51:44Z"
    hostIP: 10.128.0.45
    initContainerStatuses:
    - containerID: containerd://d5d1532037e1656ffe05ec1c14196bd4973020201887236cfb8421ca5ee2ed18
      image: docker.io/ddorgan/tellor-polkadot:dev
      imageID: docker.io/ddorgan/tellor-polkadot@sha256:bf7b47fbf006ea6bf14437326fa9b7761c01a5ae85a92bc6028a08711e7b2b95
      lastState: {}
      name: create-chainspec-rococolocal
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://d5d1532037e1656ffe05ec1c14196bd4973020201887236cfb8421ca5ee2ed18
          exitCode: 0
          finishedAt: "2023-05-17T05:51:01Z"
          reason: Completed
          startedAt: "2023-05-17T05:51:00Z"
    - containerID: containerd://80917274c0b2df99aaf7acaa678c97412aec6140c12a44eacd8ea181212605b0
      image: docker.io/ddorgan/polkadot-parachain:dev
      imageID: docker.io/ddorgan/polkadot-parachain@sha256:83cc9f7ae29e15624b3d2402ddaac928d66cf70837382dfa757e9df1891fd954
      lastState: {}
      name: create-chainspec-statemint
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://80917274c0b2df99aaf7acaa678c97412aec6140c12a44eacd8ea181212605b0
          exitCode: 0
          finishedAt: "2023-05-17T05:51:17Z"
          reason: Completed
          startedAt: "2023-05-17T05:51:17Z"
    - containerID: containerd://fe74f302192fa185d2d89d1cd43dae33fd7aef2236302ea7442f17db5f26633d
      image: docker.io/ddorgan/moonbeam:dev
      imageID: docker.io/ddorgan/moonbeam@sha256:2bb2ba16f01f3fc72a02a3ee97e83d74e02b63fbfdf0391a50105474158d3fb8
      lastState: {}
      name: create-chainspec-moonbase
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://fe74f302192fa185d2d89d1cd43dae33fd7aef2236302ea7442f17db5f26633d
          exitCode: 0
          finishedAt: "2023-05-17T05:51:33Z"
          reason: Completed
          startedAt: "2023-05-17T05:51:32Z"
    - containerID: containerd://c841c436e1963ed57e04c3815dda57f6c6831d55a89c0eb772100bb95c4981e5
      image: docker.io/ddorgan/tellor-oracle:rust-locked
      imageID: docker.io/ddorgan/tellor-oracle@sha256:ecd4753a5ed28a3015268ecc0782e186d65fe1b983bae052a7ce31c05afc2b62
      lastState: {}
      name: create-chainspec-tellor
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://c841c436e1963ed57e04c3815dda57f6c6831d55a89c0eb772100bb95c4981e5
          exitCode: 0
          finishedAt: "2023-05-17T05:51:39Z"
          reason: Completed
          startedAt: "2023-05-17T05:51:39Z"
    phase: Running
    podIP: 10.28.4.7
    podIPs:
    - ip: 10.28.4.7
    qosClass: BestEffort
    startTime: "2023-05-17T05:50:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-17T05:51:02Z"
    generateName: localrococo-bootnode-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-bootnode
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: rococo-local
      controller-revision-hash: localrococo-bootnode-5cf6df7767
      database: rocksdb
      helm.sh/chart: node-4.6.1
      pruning: archive
      release: localrococo-bootnode
      role: authority
      statefulset.kubernetes.io/pod-name: localrococo-bootnode-0
      validatorAccount: 5GNJqTPyNqANBkUVMN1LPPrxXnFouWXoe2wNSmmEoLctxiZY
    name: localrococo-bootnode-0
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-bootnode
      uid: 022c2fbf-493a-40aa-93ba-4d74193152f0
    resourceVersion: "36951488"
    uid: bf9beea3-b0ee-4859-ae50-3f76fef192ec
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        exec polkadot \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --validator \
          --database=rocksdb \
          --pruning=archive \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --node-key $(cat /custom-node-key/custom-node-key) \
          --alice \
          --ws-max-connections=5000 \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: rococo-local
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/tellor-polkadot:dev
      imagePullPolicy: Always
      name: rococo-local
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /custom-node-key/
        name: custom-node-key
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ndsr6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-bootnode-0
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ndsr6
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ndsr6
        readOnly: true
    - args:
      - /keystore
      image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
      imagePullPolicy: IfNotPresent
      name: dump-session-keys
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ndsr6
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-f7dd
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-bootnode
    serviceAccountName: localrococo-bootnode
    subdomain: localrococo-bootnode
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-bootnode-0
    - name: custom-node-key
      secret:
        defaultMode: 420
        secretName: localrococo-bootnode-custom-node-key
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - name: kube-api-access-ndsr6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:52:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T01:40:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T01:40:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:51:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://201f1b43eba246160cd25daadb7537a23903f6d69939a61d8fa625f8b7bf7899
      image: docker.io/ddorgan/tellor-polkadot:dev
      imageID: docker.io/ddorgan/tellor-polkadot@sha256:bf7b47fbf006ea6bf14437326fa9b7761c01a5ae85a92bc6028a08711e7b2b95
      lastState:
        terminated:
          containerID: containerd://9a4d7626217bb7a0d04a9ad2a62196366d33302efe45d1d3ba974d698daf8b70
          exitCode: 1
          finishedAt: "2023-05-19T01:40:22Z"
          reason: Error
          startedAt: "2023-05-18T22:20:56Z"
      name: rococo-local
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2023-05-19T01:40:24Z"
    hostIP: 10.128.0.45
    initContainerStatuses:
    - containerID: containerd://a4f123f7a0d22291ff62a96d65921f7b00e3f1c559614c2e7c4191cb06cd1f71
      image: docker.io/paritytech/lz4:latest
      imageID: docker.io/paritytech/lz4@sha256:27ecaa824c976e1a1b0871e213b72e19855904f8b62c7aeb038e3ba70f44293a
      lastState: {}
      name: download-chainspec
      ready: true
      restartCount: 2
      state:
        terminated:
          containerID: containerd://a4f123f7a0d22291ff62a96d65921f7b00e3f1c559614c2e7c4191cb06cd1f71
          exitCode: 0
          finishedAt: "2023-05-17T05:52:00Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:00Z"
    - containerID: containerd://3a7751324901315f7813f80a26fe2954d528fc73e7a88023b7d90740ed2b2395
      image: docker.io/paritytech/kubetools-kubectl:latest
      imageID: docker.io/paritytech/kubetools-kubectl@sha256:855cd83b9ff82bb9ac220fcd18d7b5d8b065cd8c13ecebfd7ac2f3488594c824
      lastState: {}
      name: retrieve-service-info
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://3a7751324901315f7813f80a26fe2954d528fc73e7a88023b7d90740ed2b2395
          exitCode: 0
          finishedAt: "2023-05-17T05:52:02Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:02Z"
    - containerID: containerd://0ccf5d2deb05d2f215a349ecfbfe92ba059567142a4430c6fbc48dda4cfd5dab
      image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
      imageID: docker.io/paritytech/substrate-session-keys-grabber@sha256:638c221aecf3e7b152d2b958f9b56acc73dca43536ae56c1379cb07768efa8c7
      lastState: {}
      name: dump-session-keys
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://0ccf5d2deb05d2f215a349ecfbfe92ba059567142a4430c6fbc48dda4cfd5dab
          exitCode: 0
          finishedAt: "2023-05-17T05:52:03Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:02Z"
    phase: Running
    podIP: 10.28.4.12
    podIPs:
    - ip: 10.28.4.12
    qosClass: BestEffort
    startTime: "2023-05-17T05:51:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-17T05:50:52Z"
    generateName: localrococo-bootnode-bob-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-bootnode-bob
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: rococo-local
      controller-revision-hash: localrococo-bootnode-bob-67bc7894b6
      database: rocksdb
      helm.sh/chart: node-4.6.1
      pruning: archive
      release: localrococo-bootnode-bob
      role: authority
      statefulset.kubernetes.io/pod-name: localrococo-bootnode-bob-0
      validatorAccount: 5HpG9w8EBLe5XCrbczpwq5TSXvedjrBGCwqxK1iQ7qUsSWFc
    name: localrococo-bootnode-bob-0
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-bootnode-bob
      uid: d77d5cf2-b726-46ef-81c3-fd7e66465ec6
    resourceVersion: "36951496"
    uid: 1f8e5ece-e541-47ac-8258-20b5a13453d8
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        exec polkadot \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --validator \
          --database=rocksdb \
          --pruning=archive \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --bob \
          --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: rococo-local
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/tellor-polkadot:dev
      imagePullPolicy: Always
      name: rococo-local
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pcm4g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-bootnode-bob-0
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pcm4g
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pcm4g
        readOnly: true
    - args:
      - /keystore
      image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
      imagePullPolicy: IfNotPresent
      name: dump-session-keys
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pcm4g
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-5nql
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-bootnode-bob
    serviceAccountName: localrococo-bootnode-bob
    subdomain: localrococo-bootnode-bob
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-bootnode-bob-0
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - name: kube-api-access-pcm4g
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:52:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T01:40:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T01:40:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:50:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://adadc0e671d1662226a0126762621acba7cb9be2a89c4fe4a1a1493ab519a5df
      image: docker.io/ddorgan/tellor-polkadot:dev
      imageID: docker.io/ddorgan/tellor-polkadot@sha256:bf7b47fbf006ea6bf14437326fa9b7761c01a5ae85a92bc6028a08711e7b2b95
      lastState:
        terminated:
          containerID: containerd://7faeb98d83409b814105e06fc38433df7ea2e5f5b633cb98a5df0a5657f9fe33
          exitCode: 1
          finishedAt: "2023-05-19T01:40:20Z"
          reason: Error
          startedAt: "2023-05-18T22:20:55Z"
      name: rococo-local
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2023-05-19T01:40:21Z"
    hostIP: 10.128.0.44
    initContainerStatuses:
    - containerID: containerd://d34ac5add2508a6dc2aced5f0387d392d7ee3c04aeaea53ad4d53d9b8f37838c
      image: docker.io/paritytech/lz4:latest
      imageID: docker.io/paritytech/lz4@sha256:27ecaa824c976e1a1b0871e213b72e19855904f8b62c7aeb038e3ba70f44293a
      lastState: {}
      name: download-chainspec
      ready: true
      restartCount: 3
      state:
        terminated:
          containerID: containerd://d34ac5add2508a6dc2aced5f0387d392d7ee3c04aeaea53ad4d53d9b8f37838c
          exitCode: 0
          finishedAt: "2023-05-17T05:52:18Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:18Z"
    - containerID: containerd://2b41ed9907c077da438481cadbb7d9e3debd40fde18069055d6eda55daf13cf2
      image: docker.io/paritytech/kubetools-kubectl:latest
      imageID: docker.io/paritytech/kubetools-kubectl@sha256:855cd83b9ff82bb9ac220fcd18d7b5d8b065cd8c13ecebfd7ac2f3488594c824
      lastState: {}
      name: retrieve-service-info
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://2b41ed9907c077da438481cadbb7d9e3debd40fde18069055d6eda55daf13cf2
          exitCode: 0
          finishedAt: "2023-05-17T05:52:19Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:19Z"
    - containerID: containerd://ece2a00dc5527dae9c153f51005b83b7334d4759e976133c1278259100b37cea
      image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
      imageID: docker.io/paritytech/substrate-session-keys-grabber@sha256:638c221aecf3e7b152d2b958f9b56acc73dca43536ae56c1379cb07768efa8c7
      lastState: {}
      name: dump-session-keys
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://ece2a00dc5527dae9c153f51005b83b7334d4759e976133c1278259100b37cea
          exitCode: 0
          finishedAt: "2023-05-17T05:52:21Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:20Z"
    phase: Running
    podIP: 10.28.3.7
    podIPs:
    - ip: 10.28.3.7
    qosClass: BestEffort
    startTime: "2023-05-17T05:50:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-19T03:38:25Z"
    generateName: localrococo-moonbase-alice-node-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-moonbase-alice
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: moonbase-local
      controller-revision-hash: localrococo-moonbase-alice-node-6999b95f54
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "2000"
      release: localrococo-moonbase-alice
      role: collator
      ss58Format: "0"
      statefulset.kubernetes.io/pod-name: localrococo-moonbase-alice-node-0
    name: localrococo-moonbase-alice-node-0
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-moonbase-alice-node
      uid: 98ada0e1-2187-4b32-b841-51b92571d191
    resourceVersion: "37015894"
    uid: 068e86d2-0554-4388-8023-99f56ac435cb
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        PARA_CHAIN_P2P_PORT="30334"
        echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
        exec /usr/local/bin/moonbeam \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --database=rocksdb \
          --collator \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --listen-addr=/ip4/0.0.0.0/tcp/30334 \
          --node-key $(cat /custom-node-key/custom-node-key) \
          --alice \
          --ws-max-connections=5000 \
          -- \
          --name=${POD_NAME} \
          --base-path=/relaychain-data \
          --keystore-path=/relaychain-keystore \
          --database=rocksdb \
          --chain=/relaychain-data/relay_chain_chainspec.json \
          --bootnodes /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: moonbase-local
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/moonbeam:dev
      imagePullPolicy: Always
      name: moonbase-local
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /relaychain-keystore
        name: relaychain-keystore
      - mountPath: /custom-node-key/
        name: custom-node-key
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c8zpc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-moonbase-alice-node-0
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/moonbase.json
          wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c8zpc
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c8zpc
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-5nql
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-moonbase-alice-node
    serviceAccountName: localrococo-moonbase-alice-node
    subdomain: localrococo-moonbase-alice-node
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-moonbase-alice-node-0
    - name: relaychain-data
      persistentVolumeClaim:
        claimName: relaychain-data-localrococo-moonbase-alice-node-0
    - name: custom-node-key
      secret:
        defaultMode: 420
        secretName: localrococo-moonbase-alice-node-custom-node-key
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: relaychain-keystore
    - name: kube-api-access-c8zpc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:42:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:42:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://82fcc104dd16583a01c3c0a16c58b75cdbe079cbd84ce9e1719f17bbbee06dd0
      image: docker.io/ddorgan/moonbeam:dev
      imageID: docker.io/ddorgan/moonbeam@sha256:2bb2ba16f01f3fc72a02a3ee97e83d74e02b63fbfdf0391a50105474158d3fb8
      lastState:
        terminated:
          containerID: containerd://1ae3a725e7a28e0d224df16873297e9f24952c2e4f195e72cd544fc3a5b2cd14
          exitCode: 1
          finishedAt: "2023-05-19T03:39:50Z"
          reason: Error
          startedAt: "2023-05-19T03:39:46Z"
      name: moonbase-local
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2023-05-19T03:40:35Z"
    hostIP: 10.128.0.44
    initContainerStatuses:
    - containerID: containerd://c5483f5228df955b8e2c1d13a54539fd155368b13a91b00dedd88a8b2e5743b2
      image: docker.io/paritytech/lz4:latest
      imageID: docker.io/paritytech/lz4@sha256:27ecaa824c976e1a1b0871e213b72e19855904f8b62c7aeb038e3ba70f44293a
      lastState: {}
      name: download-chainspec
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://c5483f5228df955b8e2c1d13a54539fd155368b13a91b00dedd88a8b2e5743b2
          exitCode: 0
          finishedAt: "2023-05-19T03:38:41Z"
          reason: Completed
          startedAt: "2023-05-19T03:38:40Z"
    - containerID: containerd://bbad6ee8f2bac04adeb4e9904516edb74fb07ecba6a867b72d393a649ebf4282
      image: docker.io/paritytech/kubetools-kubectl:latest
      imageID: docker.io/paritytech/kubetools-kubectl@sha256:855cd83b9ff82bb9ac220fcd18d7b5d8b065cd8c13ecebfd7ac2f3488594c824
      lastState: {}
      name: retrieve-service-info
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://bbad6ee8f2bac04adeb4e9904516edb74fb07ecba6a867b72d393a649ebf4282
          exitCode: 0
          finishedAt: "2023-05-19T03:38:41Z"
          reason: Completed
          startedAt: "2023-05-19T03:38:41Z"
    phase: Running
    podIP: 10.28.3.50
    podIPs:
    - ip: 10.28.3.50
    qosClass: BestEffort
    startTime: "2023-05-19T03:38:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-19T03:06:40Z"
    generateName: localrococo-moonbase-collator-node-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-moonbase-collator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: moonbase-local
      controller-revision-hash: localrococo-moonbase-collator-node-869bb577cf
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "2000"
      pruning: archive
      release: localrococo-moonbase-collator
      role: collator
      ss58Format: "0"
      statefulset.kubernetes.io/pod-name: localrococo-moonbase-collator-node-0
    name: localrococo-moonbase-collator-node-0
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-moonbase-collator-node
      uid: 5fb8ca7b-8396-40b3-8020-f1ec31d503a3
    resourceVersion: "36997627"
    uid: 60857adc-d93a-4879-98ae-a2b7c8a7a9f1
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        PARA_CHAIN_P2P_PORT="30334"
        echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
        exec /usr/local/bin/moonbeam \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --database=rocksdb \
          --pruning=archive \
          --collator \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --listen-addr=/ip4/0.0.0.0/tcp/30334 \
          --bootnodes  /dns4/localrococo-moonbase-alice-node-0/tcp/30334/p2p/12D3KooWCGmHA8TkwvxhCXhQgeragffknJL4qN1YQh4AwsE9EL4C \
          -- \
          --name=${POD_NAME} \
          --base-path=/relaychain-data \
          --keystore-path=/relaychain-keystore \
          --database=rocksdb \
          --pruning=1000 \
          --chain=/relaychain-data/relay_chain_chainspec.json \
          --bootnodes /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: moonbase-local
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/moonbeam:dev
      imagePullPolicy: Always
      name: moonbase-local
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /relaychain-keystore
        name: relaychain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2j7k2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-moonbase-collator-node-0
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/moonbase.json
          wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2j7k2
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2j7k2
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-57s8
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-moonbase-collator-node
    serviceAccountName: localrococo-moonbase-collator-node
    subdomain: localrococo-moonbase-collator-node
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-moonbase-collator-node-0
    - name: relaychain-data
      persistentVolumeClaim:
        claimName: relaychain-data-localrococo-moonbase-collator-node-0
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: relaychain-keystore
    - name: kube-api-access-2j7k2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:07:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:07:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:07:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:06:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://829a46e2dc0af38538a576cb98c657127e4cacb4e2978c1013e147e6a57088a1
      image: docker.io/ddorgan/moonbeam:dev
      imageID: docker.io/ddorgan/moonbeam@sha256:2bb2ba16f01f3fc72a02a3ee97e83d74e02b63fbfdf0391a50105474158d3fb8
      lastState: {}
      name: moonbase-local
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-05-19T03:07:02Z"
    hostIP: 10.128.0.43
    initContainerStatuses:
    - containerID: containerd://fb2e593145fbe0ae14e871007ae6d266508663833f957a21dc3126b6e41861dc
      image: docker.io/paritytech/lz4:latest
      imageID: docker.io/paritytech/lz4@sha256:27ecaa824c976e1a1b0871e213b72e19855904f8b62c7aeb038e3ba70f44293a
      lastState: {}
      name: download-chainspec
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://fb2e593145fbe0ae14e871007ae6d266508663833f957a21dc3126b6e41861dc
          exitCode: 0
          finishedAt: "2023-05-19T03:07:00Z"
          reason: Completed
          startedAt: "2023-05-19T03:06:59Z"
    - containerID: containerd://dc750969b94c994936cafb3a50fa02636e6432a976019eef096e9240180816b8
      image: docker.io/paritytech/kubetools-kubectl:latest
      imageID: docker.io/paritytech/kubetools-kubectl@sha256:855cd83b9ff82bb9ac220fcd18d7b5d8b065cd8c13ecebfd7ac2f3488594c824
      lastState: {}
      name: retrieve-service-info
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://dc750969b94c994936cafb3a50fa02636e6432a976019eef096e9240180816b8
          exitCode: 0
          finishedAt: "2023-05-19T03:07:01Z"
          reason: Completed
          startedAt: "2023-05-19T03:07:01Z"
    phase: Running
    podIP: 10.28.2.31
    podIPs:
    - ip: 10.28.2.31
    qosClass: BestEffort
    startTime: "2023-05-19T03:06:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-17T05:50:58Z"
    generateName: localrococo-statemint-alice-node-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-statemint-alice
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: statemint-local
      collatorAccount: 15oF4uVJwmo4TdGW7VfQxNLavjCXviqxT9S1MgbjMNHr6Sp5
      controller-revision-hash: localrococo-statemint-alice-node-6cf4d55d66
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "1000"
      release: localrococo-statemint-alice
      role: collator
      ss58Format: "0"
      statefulset.kubernetes.io/pod-name: localrococo-statemint-alice-node-0
    name: localrococo-statemint-alice-node-0
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-statemint-alice-node
      uid: c97a1174-9d3b-4225-bca5-a86f5cfdc8b9
    resourceVersion: "37021382"
    uid: 2e5f2459-de14-498d-b091-d741409e11e2
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        PARA_CHAIN_P2P_PORT="30334"
        echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
        exec /usr/local/bin/polkadot-parachain \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --database=rocksdb \
          --collator \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --listen-addr=/ip4/0.0.0.0/tcp/30334 \
          --node-key $(cat /custom-node-key/custom-node-key) \
          --alice \
          --ws-max-connections=5000 \
          -- \
          --name=${POD_NAME} \
          --base-path=/relaychain-data \
          --keystore-path=/relaychain-keystore \
          --database=rocksdb \
          --chain=/relaychain-data/relay_chain_chainspec.json \
          --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: statemint-local
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/polkadot-parachain:dev
      imagePullPolicy: Always
      name: statemint-local
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /relaychain-keystore
        name: relaychain-keystore
      - mountPath: /custom-node-key/
        name: custom-node-key
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p59vm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-statemint-alice-node-0
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/statemint.json
          wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p59vm
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p59vm
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-57s8
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-statemint-alice-node
    serviceAccountName: localrococo-statemint-alice-node
    subdomain: localrococo-statemint-alice-node
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-statemint-alice-node-0
    - name: relaychain-data
      persistentVolumeClaim:
        claimName: relaychain-data-localrococo-statemint-alice-node-0
    - name: custom-node-key
      secret:
        defaultMode: 420
        secretName: localrococo-statemint-alice-node-custom-node-key
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: relaychain-keystore
    - name: kube-api-access-p59vm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:52:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:53:06Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:53:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:50:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e769e59e9a19cb15f2d08c5600d8adb5584759f603bdf5c6a95759a76b39f3d8
      image: docker.io/ddorgan/polkadot-parachain:dev
      imageID: docker.io/ddorgan/polkadot-parachain@sha256:83cc9f7ae29e15624b3d2402ddaac928d66cf70837382dfa757e9df1891fd954
      lastState:
        terminated:
          containerID: containerd://b36f72247ea4023d8f1263d4c3696ab56177387fe7754b4133257c2ffda37836
          exitCode: 1
          finishedAt: "2023-05-19T03:46:38Z"
          reason: Error
          startedAt: "2023-05-19T03:46:34Z"
      name: statemint-local
      ready: true
      restartCount: 219
      started: true
      state:
        running:
          startedAt: "2023-05-19T03:51:50Z"
    hostIP: 10.128.0.43
    initContainerStatuses:
    - containerID: containerd://3f7995d7214d5877d8a69a365d82d1c6fbb28800e250a8aebcee78af5e718ab2
      image: docker.io/paritytech/lz4:latest
      imageID: docker.io/paritytech/lz4@sha256:27ecaa824c976e1a1b0871e213b72e19855904f8b62c7aeb038e3ba70f44293a
      lastState: {}
      name: download-chainspec
      ready: true
      restartCount: 3
      state:
        terminated:
          containerID: containerd://3f7995d7214d5877d8a69a365d82d1c6fbb28800e250a8aebcee78af5e718ab2
          exitCode: 0
          finishedAt: "2023-05-17T05:52:18Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:18Z"
    - containerID: containerd://cae9298c80ab45e718a879480978ba43d7996f6b74a91806820f65e64cb07eec
      image: docker.io/paritytech/kubetools-kubectl:latest
      imageID: docker.io/paritytech/kubetools-kubectl@sha256:855cd83b9ff82bb9ac220fcd18d7b5d8b065cd8c13ecebfd7ac2f3488594c824
      lastState: {}
      name: retrieve-service-info
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://cae9298c80ab45e718a879480978ba43d7996f6b74a91806820f65e64cb07eec
          exitCode: 0
          finishedAt: "2023-05-17T05:52:27Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:27Z"
    phase: Running
    podIP: 10.28.2.11
    podIPs:
    - ip: 10.28.2.11
    qosClass: BestEffort
    startTime: "2023-05-17T05:50:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-19T03:58:47Z"
    generateName: localrococo-statemint-bob-node-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-statemint-bob
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: statemint-local
      controller-revision-hash: localrococo-statemint-bob-node-79bf47c79c
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "1000"
      release: localrococo-statemint-bob
      role: collator
      ss58Format: "0"
      statefulset.kubernetes.io/pod-name: localrococo-statemint-bob-node-0
    name: localrococo-statemint-bob-node-0
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-statemint-bob-node
      uid: 9f540e44-c36f-4917-b768-41f3c9244827
    resourceVersion: "37024299"
    uid: 096cb345-6298-40c5-9a0f-4b66a5e48136
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        PARA_CHAIN_P2P_PORT="30334"
        echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
        exec /usr/local/bin/polkadot-parachain \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --database=rocksdb \
          --collator \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --listen-addr=/ip4/0.0.0.0/tcp/30334 \
          --bob \
          --bootnodes  /dns4/localrococo-statemint-alice-node-0/tcp/30334/p2p/12D3KooWJsfMoQmEgsWK99oxUbFYFpwXnupQMJCDWW9AqARQ1CqM \
          -- \
          --name=${POD_NAME} \
          --base-path=/relaychain-data \
          --keystore-path=/relaychain-keystore \
          --database=rocksdb \
          --chain=/relaychain-data/relay_chain_chainspec.json \
          --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: statemint-local
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/polkadot-parachain:dev
      imagePullPolicy: Always
      name: statemint-local
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /relaychain-keystore
        name: relaychain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-825gg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-statemint-bob-node-0
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/statemint.json
          wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-825gg
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-825gg
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-57s8
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-statemint-bob-node
    serviceAccountName: localrococo-statemint-bob-node
    subdomain: localrococo-statemint-bob-node
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-statemint-bob-node-0
    - name: relaychain-data
      persistentVolumeClaim:
        claimName: relaychain-data-localrococo-statemint-bob-node-0
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: relaychain-keystore
    - name: kube-api-access-825gg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:58:47Z"
      message: 'containers with incomplete status: [download-chainspec retrieve-service-info]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:58:47Z"
      message: 'containers with unready status: [statemint-local]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:58:47Z"
      message: 'containers with unready status: [statemint-local]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:58:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: ddorgan/polkadot-parachain:dev
      imageID: ""
      lastState: {}
      name: statemint-local
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
    hostIP: 10.128.0.43
    initContainerStatuses:
    - image: paritytech/lz4:latest
      imageID: ""
      lastState: {}
      name: download-chainspec
      ready: false
      restartCount: 0
      state:
        waiting:
          reason: PodInitializing
    - image: paritytech/kubetools-kubectl:latest
      imageID: ""
      lastState: {}
      name: retrieve-service-info
      ready: false
      restartCount: 0
      state:
        waiting:
          reason: PodInitializing
    phase: Pending
    qosClass: BestEffort
    startTime: "2023-05-19T03:58:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-17T05:50:55Z"
    generateName: localrococo-tellor-alice-node-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-tellor-alice
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: rust-locked
      chain: polkadot
      controller-revision-hash: localrococo-tellor-alice-node-5c5cd664f5
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "3000"
      release: localrococo-tellor-alice
      role: collator
      statefulset.kubernetes.io/pod-name: localrococo-tellor-alice-node-0
    name: localrococo-tellor-alice-node-0
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-tellor-alice-node
      uid: b04b97ce-43a0-458e-938b-e1a04ff976f0
    resourceVersion: "37022762"
    uid: 5962181c-bd00-4355-9039-cce3aa7a00b1
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        PARA_CHAIN_P2P_PORT="30334"
        echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
        exec /usr/local/bin/parachain-template-node \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --database=rocksdb \
          --collator \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --listen-addr=/ip4/0.0.0.0/tcp/30334 \
          --node-key $(cat /custom-node-key/custom-node-key) \
          --alice \
          --ws-max-connections=5000 \
          -- \
          --name=${POD_NAME} \
          --base-path=/relaychain-data \
          --keystore-path=/relaychain-keystore \
          --database=rocksdb \
          --chain=/relaychain-data/relay_chain_chainspec.json \
          --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: polkadot
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/tellor-oracle:rust-locked
      imagePullPolicy: Always
      name: polkadot
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /relaychain-keystore
        name: relaychain-keystore
      - mountPath: /custom-node-key/
        name: custom-node-key
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qmfff
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-tellor-alice-node-0
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/tellor.json
          wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qmfff
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qmfff
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-5nql
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-tellor-alice-node
    serviceAccountName: localrococo-tellor-alice-node
    subdomain: localrococo-tellor-alice-node
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-tellor-alice-node-0
    - name: relaychain-data
      persistentVolumeClaim:
        claimName: relaychain-data-localrococo-tellor-alice-node-0
    - name: custom-node-key
      secret:
        defaultMode: 420
        secretName: localrococo-tellor-alice-node-custom-node-key
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: relaychain-keystore
    - name: kube-api-access-qmfff
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:52:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-18T09:35:20Z"
      message: 'containers with unready status: [polkadot]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-18T09:35:20Z"
      message: 'containers with unready status: [polkadot]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:50:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9f6d7580fcfa82927e3b98c3e37294f87bb9b327c2001407345b38bdeb6ef638
      image: docker.io/ddorgan/tellor-oracle:rust-locked
      imageID: docker.io/ddorgan/tellor-oracle@sha256:ecd4753a5ed28a3015268ecc0782e186d65fe1b983bae052a7ce31c05afc2b62
      lastState:
        terminated:
          containerID: containerd://9f6d7580fcfa82927e3b98c3e37294f87bb9b327c2001407345b38bdeb6ef638
          exitCode: 1
          finishedAt: "2023-05-19T03:55:47Z"
          reason: Error
          startedAt: "2023-05-19T03:55:41Z"
      name: polkadot
      ready: false
      restartCount: 219
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=polkadot pod=localrococo-tellor-alice-node-0_rococo(5962181c-bd00-4355-9039-cce3aa7a00b1)
          reason: CrashLoopBackOff
    hostIP: 10.128.0.44
    initContainerStatuses:
    - containerID: containerd://ede8e55758dbca696b4dfae736d9dae4c1e2dde20f0207ed6b1834d2eb37c478
      image: docker.io/paritytech/lz4:latest
      imageID: docker.io/paritytech/lz4@sha256:27ecaa824c976e1a1b0871e213b72e19855904f8b62c7aeb038e3ba70f44293a
      lastState: {}
      name: download-chainspec
      ready: true
      restartCount: 2
      state:
        terminated:
          containerID: containerd://ede8e55758dbca696b4dfae736d9dae4c1e2dde20f0207ed6b1834d2eb37c478
          exitCode: 0
          finishedAt: "2023-05-17T05:52:02Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:02Z"
    - containerID: containerd://a9d2c4b3c83716660cf4091fa2bf151da3020290f6018586a434ffd0cdade9d2
      image: docker.io/paritytech/kubetools-kubectl:latest
      imageID: docker.io/paritytech/kubetools-kubectl@sha256:855cd83b9ff82bb9ac220fcd18d7b5d8b065cd8c13ecebfd7ac2f3488594c824
      lastState: {}
      name: retrieve-service-info
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://a9d2c4b3c83716660cf4091fa2bf151da3020290f6018586a434ffd0cdade9d2
          exitCode: 0
          finishedAt: "2023-05-17T05:52:12Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:12Z"
    phase: Running
    podIP: 10.28.3.10
    podIPs:
    - ip: 10.28.3.10
    qosClass: BestEffort
    startTime: "2023-05-17T05:50:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-17T05:50:52Z"
    generateName: localrococo-tellor-bob-node-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-tellor-bob
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: rust-locked
      chain: polkadot
      controller-revision-hash: localrococo-tellor-bob-node-577454f9b8
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "3000"
      release: localrococo-tellor-bob
      role: collator
      statefulset.kubernetes.io/pod-name: localrococo-tellor-bob-node-0
    name: localrococo-tellor-bob-node-0
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-tellor-bob-node
      uid: f1c976cf-1a5e-458b-8b16-6ea37b133d9c
    resourceVersion: "37022053"
    uid: f5d5a999-fb63-4bb9-bbcf-c37b4cf81c78
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        PARA_CHAIN_P2P_PORT="30334"
        echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
        exec /usr/local/bin/parachain-template-node \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --database=rocksdb \
          --collator \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --listen-addr=/ip4/0.0.0.0/tcp/30334 \
          --bob \
          --bootnodes  /dns4/localrococo-tellor-alice-node-0/tcp/30334/p2p/12D3KooWRBfVfhHEjbLvrFU9koaWwxZHvtaiMkyYnwHnfwoiTwoY \
          -- \
          --name=${POD_NAME} \
          --base-path=/relaychain-data \
          --keystore-path=/relaychain-keystore \
          --database=rocksdb \
          --chain=/relaychain-data/relay_chain_chainspec.json \
          --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: polkadot
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/tellor-oracle:rust-locked
      imagePullPolicy: Always
      name: polkadot
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /relaychain-keystore
        name: relaychain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rqhvf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-tellor-bob-node-0
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/tellor.json
          wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /relaychain-data
        name: relaychain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rqhvf
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rqhvf
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-5nql
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-tellor-bob-node
    serviceAccountName: localrococo-tellor-bob-node
    subdomain: localrococo-tellor-bob-node
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-tellor-bob-node-0
    - name: relaychain-data
      persistentVolumeClaim:
        claimName: relaychain-data-localrococo-tellor-bob-node-0
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: relaychain-keystore
    - name: kube-api-access-rqhvf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:52:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-18T09:35:20Z"
      message: 'containers with unready status: [polkadot]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-18T09:35:20Z"
      message: 'containers with unready status: [polkadot]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:50:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9f12a9f55d726540124774454988a4331038ae954695ec2ea8d98b4d1a21a9ee
      image: docker.io/ddorgan/tellor-oracle:rust-locked
      imageID: docker.io/ddorgan/tellor-oracle@sha256:ecd4753a5ed28a3015268ecc0782e186d65fe1b983bae052a7ce31c05afc2b62
      lastState:
        terminated:
          containerID: containerd://9f12a9f55d726540124774454988a4331038ae954695ec2ea8d98b4d1a21a9ee
          exitCode: 1
          finishedAt: "2023-05-19T03:54:25Z"
          reason: Error
          startedAt: "2023-05-19T03:54:18Z"
      name: polkadot
      ready: false
      restartCount: 219
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=polkadot pod=localrococo-tellor-bob-node-0_rococo(f5d5a999-fb63-4bb9-bbcf-c37b4cf81c78)
          reason: CrashLoopBackOff
    hostIP: 10.128.0.44
    initContainerStatuses:
    - containerID: containerd://4e3b49e34ab59bdbf80de6b03582d556021108fa27708db77b8d584dbc27b47f
      image: docker.io/paritytech/lz4:latest
      imageID: docker.io/paritytech/lz4@sha256:27ecaa824c976e1a1b0871e213b72e19855904f8b62c7aeb038e3ba70f44293a
      lastState: {}
      name: download-chainspec
      ready: true
      restartCount: 2
      state:
        terminated:
          containerID: containerd://4e3b49e34ab59bdbf80de6b03582d556021108fa27708db77b8d584dbc27b47f
          exitCode: 0
          finishedAt: "2023-05-17T05:51:52Z"
          reason: Completed
          startedAt: "2023-05-17T05:51:52Z"
    - containerID: containerd://570e3d7dd107ed3f1f8108d2e653dc105ec48ae1ccff59209c5aa4cbf176c21d
      image: docker.io/paritytech/kubetools-kubectl:latest
      imageID: docker.io/paritytech/kubetools-kubectl@sha256:855cd83b9ff82bb9ac220fcd18d7b5d8b065cd8c13ecebfd7ac2f3488594c824
      lastState: {}
      name: retrieve-service-info
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://570e3d7dd107ed3f1f8108d2e653dc105ec48ae1ccff59209c5aa4cbf176c21d
          exitCode: 0
          finishedAt: "2023-05-17T05:52:01Z"
          reason: Completed
          startedAt: "2023-05-17T05:52:01Z"
    phase: Running
    podIP: 10.28.3.8
    podIPs:
    - ip: 10.28.3.8
    qosClass: BestEffort
    startTime: "2023-05-17T05:50:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-19T02:57:47Z"
    generateName: localrococo-validator-a-node-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-validator-a
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: rococo-local
      controller-revision-hash: localrococo-validator-a-node-6cd9d86dd5
      database: rocksdb
      helm.sh/chart: node-4.6.1
      pruning: archive
      release: localrococo-validator-a
      role: authority
      statefulset.kubernetes.io/pod-name: localrococo-validator-a-node-0
    name: localrococo-validator-a-node-0
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-validator-a-node
      uid: 54e73c5a-6d46-4e34-ade7-dad1c2a50e09
    resourceVersion: "36992695"
    uid: e6476192-51c1-4b25-bf07-9cad74c47572
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        exec polkadot \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --validator \
          --database=rocksdb \
          --pruning=archive \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: rococo-local
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/tellor-polkadot:dev
      imagePullPolicy: Always
      name: rococo-local
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-54p5b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-validator-a-node-0
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-54p5b
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        if [ ! -f /var/run/secrets/gran/type ]; then
           echo "Error: File /var/run/secrets/gran/type does not exist"
           exit 1
        fi
        polkadot key insert \
        --keystore-path /keystore \
        --key-type $(cat /var/run/secrets/gran/type) \
        --scheme $(cat /var/run/secrets/gran/scheme) \
        --suri "$(cat /var/run/secrets/gran/seed)//validator//${HOSTNAME}" \
        && echo "Inserted key gran into Keystore" \
        || echo "Failed to insert key gran into Keystore."
        if [ ! -f /var/run/secrets/babe/type ]; then
           echo "Error: File /var/run/secrets/babe/type does not exist"
           exit 1
        fi
        polkadot key insert \
        --keystore-path /keystore \
        --key-type $(cat /var/run/secrets/babe/type) \
        --scheme $(cat /var/run/secrets/babe/scheme) \
        --suri "$(cat /var/run/secrets/babe/seed)//validator//${HOSTNAME}" \
        && echo "Inserted key babe into Keystore" \
        || echo "Failed to insert key babe into Keystore."
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: rococo-local
      image: ddorgan/tellor-polkadot:dev
      imagePullPolicy: IfNotPresent
      name: inject-keys
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /var/run/secrets/gran
        name: gran
      - mountPath: /var/run/secrets/babe
        name: babe
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-54p5b
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-54p5b
        readOnly: true
    - args:
      - /keystore
      image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
      imagePullPolicy: IfNotPresent
      name: dump-session-keys
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-54p5b
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-5nql
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-validator-a-node
    serviceAccountName: localrococo-validator-a-node
    subdomain: localrococo-validator-a-node
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-validator-a-node-0
    - name: gran
      secret:
        defaultMode: 256
        secretName: localrococo-validator-a-node-gran
    - name: babe
      secret:
        defaultMode: 256
        secretName: localrococo-validator-a-node-babe
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - name: kube-api-access-54p5b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T02:58:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T02:58:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T02:58:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T02:57:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e94496da29131dcccf91484d2fcd3d174a04e2550fbb6f28630f8f213ed72d38
      image: docker.io/ddorgan/tellor-polkadot:dev
      imageID: docker.io/ddorgan/tellor-polkadot@sha256:bf7b47fbf006ea6bf14437326fa9b7761c01a5ae85a92bc6028a08711e7b2b95
      lastState: {}
      name: rococo-local
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-05-19T02:58:03Z"
    hostIP: 10.128.0.44
    initContainerStatuses:
    - containerID: containerd://0f15a1c955df1de5e9380c722a5afd65fd683f4ff286bf495474f1dda990264c
      image: docker.io/paritytech/lz4:latest
      imageID: docker.io/paritytech/lz4@sha256:27ecaa824c976e1a1b0871e213b72e19855904f8b62c7aeb038e3ba70f44293a
      lastState: {}
      name: download-chainspec
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://0f15a1c955df1de5e9380c722a5afd65fd683f4ff286bf495474f1dda990264c
          exitCode: 0
          finishedAt: "2023-05-19T02:58:00Z"
          reason: Completed
          startedAt: "2023-05-19T02:57:59Z"
    - containerID: containerd://b13af6989995520edcb1609854dc71ccc194698984d07ac6d56d7797bb51cd0f
      image: docker.io/ddorgan/tellor-polkadot:dev
      imageID: docker.io/ddorgan/tellor-polkadot@sha256:bf7b47fbf006ea6bf14437326fa9b7761c01a5ae85a92bc6028a08711e7b2b95
      lastState: {}
      name: inject-keys
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://b13af6989995520edcb1609854dc71ccc194698984d07ac6d56d7797bb51cd0f
          exitCode: 0
          finishedAt: "2023-05-19T02:58:00Z"
          reason: Completed
          startedAt: "2023-05-19T02:58:00Z"
    - containerID: containerd://4008e0c207f1edbcc6fd0f23c78cc70b3cd6212cb86d985adef0bc8fc9013d9d
      image: docker.io/paritytech/kubetools-kubectl:latest
      imageID: docker.io/paritytech/kubetools-kubectl@sha256:855cd83b9ff82bb9ac220fcd18d7b5d8b065cd8c13ecebfd7ac2f3488594c824
      lastState: {}
      name: retrieve-service-info
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://4008e0c207f1edbcc6fd0f23c78cc70b3cd6212cb86d985adef0bc8fc9013d9d
          exitCode: 0
          finishedAt: "2023-05-19T02:58:01Z"
          reason: Completed
          startedAt: "2023-05-19T02:58:01Z"
    - containerID: containerd://fb6f4cc94e6fa8aa6813a81201e2cd8c9baa0bf07adb054fa56fc35457ec4686
      image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
      imageID: docker.io/paritytech/substrate-session-keys-grabber@sha256:638c221aecf3e7b152d2b958f9b56acc73dca43536ae56c1379cb07768efa8c7
      lastState: {}
      name: dump-session-keys
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://fb6f4cc94e6fa8aa6813a81201e2cd8c9baa0bf07adb054fa56fc35457ec4686
          exitCode: 0
          finishedAt: "2023-05-19T02:58:02Z"
          reason: Completed
          startedAt: "2023-05-19T02:58:02Z"
    phase: Running
    podIP: 10.28.3.32
    podIPs:
    - ip: 10.28.3.32
    qosClass: BestEffort
    startTime: "2023-05-19T02:57:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-19T02:57:47Z"
    generateName: localrococo-validator-a-node-
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-validator-a
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: rococo-local
      controller-revision-hash: localrococo-validator-a-node-6cd9d86dd5
      database: rocksdb
      helm.sh/chart: node-4.6.1
      pruning: archive
      release: localrococo-validator-a
      role: authority
      statefulset.kubernetes.io/pod-name: localrococo-validator-a-node-1
    name: localrococo-validator-a-node-1
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: localrococo-validator-a-node
      uid: 54e73c5a-6d46-4e34-ade7-dad1c2a50e09
    resourceVersion: "36992688"
    uid: 3ac21750-d538-46cf-8e3f-af57e5785549
  spec:
    containers:
    - args:
      - -c
      - |
        set -eu
        POD_INDEX="${HOSTNAME##*-}"
        RELAY_CHAIN_P2P_PORT="30333"
        echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
        exec polkadot \
          --name=${POD_NAME} \
          --base-path=/chain-data \
          --keystore-path=/keystore \
          --chain=/chain-data/chainspec.json \
          --validator \
          --database=rocksdb \
          --pruning=archive \
          --prometheus-external \
          --prometheus-port 9615 \
          --unsafe-rpc-external \
          --unsafe-ws-external \
          --rpc-cors=all \
          --rpc-methods=unsafe \
          --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
          --listen-addr=/ip4/0.0.0.0/tcp/30333 \
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: rococo-local
      - name: NODE_NAME
        value: $(POD_NAME)
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ddorgan/tellor-polkadot:dev
      imagePullPolicy: Always
      name: rococo-local
      ports:
      - containerPort: 9933
        name: http-rpc
        protocol: TCP
      - containerPort: 9944
        name: websocket-rpc
        protocol: TCP
      - containerPort: 9615
        name: prometheus
        protocol: TCP
      - containerPort: 30333
        name: p2p
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http-rpc
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8lfx6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: localrococo-validator-a-node-1
    initContainers:
    - args:
      - -c
      - |
        set -eu -o pipefail -x
          wget -O /chain-data/chainspec.json http://chainspec.rococo/rococo-local.json
      command:
      - /bin/sh
      image: paritytech/lz4:latest
      imagePullPolicy: Always
      name: download-chainspec
      resources: {}
      securityContext:
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8lfx6
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        if [ ! -f /var/run/secrets/gran/type ]; then
           echo "Error: File /var/run/secrets/gran/type does not exist"
           exit 1
        fi
        polkadot key insert \
        --keystore-path /keystore \
        --key-type $(cat /var/run/secrets/gran/type) \
        --scheme $(cat /var/run/secrets/gran/scheme) \
        --suri "$(cat /var/run/secrets/gran/seed)//validator//${HOSTNAME}" \
        && echo "Inserted key gran into Keystore" \
        || echo "Failed to insert key gran into Keystore."
        if [ ! -f /var/run/secrets/babe/type ]; then
           echo "Error: File /var/run/secrets/babe/type does not exist"
           exit 1
        fi
        polkadot key insert \
        --keystore-path /keystore \
        --key-type $(cat /var/run/secrets/babe/type) \
        --scheme $(cat /var/run/secrets/babe/scheme) \
        --suri "$(cat /var/run/secrets/babe/seed)//validator//${HOSTNAME}" \
        && echo "Inserted key babe into Keystore" \
        || echo "Failed to insert key babe into Keystore."
      command:
      - /bin/sh
      env:
      - name: CHAIN
        value: rococo-local
      image: ddorgan/tellor-polkadot:dev
      imagePullPolicy: IfNotPresent
      name: inject-keys
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /var/run/secrets/gran
        name: gran
      - mountPath: /var/run/secrets/babe
        name: babe
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8lfx6
        readOnly: true
    - args:
      - -c
      - |
        set -eu -x
        POD_INDEX="${HOSTNAME##*-}"
      command:
      - /bin/sh
      image: paritytech/kubetools-kubectl:latest
      imagePullPolicy: Always
      name: retrieve-service-info
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /chain-data
        name: chain-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8lfx6
        readOnly: true
    - args:
      - /keystore
      image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
      imagePullPolicy: IfNotPresent
      name: dump-session-keys
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /keystore
        name: chain-keystore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8lfx6
        readOnly: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-f7dd
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: localrococo-validator-a-node
    serviceAccountName: localrococo-validator-a-node
    subdomain: localrococo-validator-a-node
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: chain-data
      persistentVolumeClaim:
        claimName: chain-data-localrococo-validator-a-node-1
    - name: gran
      secret:
        defaultMode: 256
        secretName: localrococo-validator-a-node-gran
    - name: babe
      secret:
        defaultMode: 256
        secretName: localrococo-validator-a-node-babe
    - emptyDir:
        medium: Memory
        sizeLimit: 10Mi
      name: chain-keystore
    - name: kube-api-access-8lfx6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T02:58:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T02:58:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T02:58:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T02:57:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://302682982dd25a68a67fd93570d0a428f340d91c8301c56467b7817398807b13
      image: docker.io/ddorgan/tellor-polkadot:dev
      imageID: docker.io/ddorgan/tellor-polkadot@sha256:bf7b47fbf006ea6bf14437326fa9b7761c01a5ae85a92bc6028a08711e7b2b95
      lastState: {}
      name: rococo-local
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-05-19T02:58:01Z"
    hostIP: 10.128.0.45
    initContainerStatuses:
    - containerID: containerd://b608219e6c1f430ccd0b0bf52359e76097388ea6fb87b5a74e120c149a786dfa
      image: docker.io/paritytech/lz4:latest
      imageID: docker.io/paritytech/lz4@sha256:27ecaa824c976e1a1b0871e213b72e19855904f8b62c7aeb038e3ba70f44293a
      lastState: {}
      name: download-chainspec
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://b608219e6c1f430ccd0b0bf52359e76097388ea6fb87b5a74e120c149a786dfa
          exitCode: 0
          finishedAt: "2023-05-19T02:57:58Z"
          reason: Completed
          startedAt: "2023-05-19T02:57:58Z"
    - containerID: containerd://1f79352787259174632f3b5cb8fbedc3debd8ab35260ee5c8f17414eb11c6657
      image: docker.io/ddorgan/tellor-polkadot:dev
      imageID: docker.io/ddorgan/tellor-polkadot@sha256:bf7b47fbf006ea6bf14437326fa9b7761c01a5ae85a92bc6028a08711e7b2b95
      lastState: {}
      name: inject-keys
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://1f79352787259174632f3b5cb8fbedc3debd8ab35260ee5c8f17414eb11c6657
          exitCode: 0
          finishedAt: "2023-05-19T02:57:58Z"
          reason: Completed
          startedAt: "2023-05-19T02:57:58Z"
    - containerID: containerd://e86e9c727f57945dcf14644e5ba347fc9b1ef2b6a333ee0e82392adcf29c1072
      image: docker.io/paritytech/kubetools-kubectl:latest
      imageID: docker.io/paritytech/kubetools-kubectl@sha256:855cd83b9ff82bb9ac220fcd18d7b5d8b065cd8c13ecebfd7ac2f3488594c824
      lastState: {}
      name: retrieve-service-info
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://e86e9c727f57945dcf14644e5ba347fc9b1ef2b6a333ee0e82392adcf29c1072
          exitCode: 0
          finishedAt: "2023-05-19T02:57:59Z"
          reason: Completed
          startedAt: "2023-05-19T02:57:59Z"
    - containerID: containerd://680284d74ff4101613f66821fc4dcc099f0752a0991fbf1353f164cfe051cb6c
      image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
      imageID: docker.io/paritytech/substrate-session-keys-grabber@sha256:638c221aecf3e7b152d2b958f9b56acc73dca43536ae56c1379cb07768efa8c7
      lastState: {}
      name: dump-session-keys
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://680284d74ff4101613f66821fc4dcc099f0752a0991fbf1353f164cfe051cb6c
          exitCode: 0
          finishedAt: "2023-05-19T02:58:00Z"
          reason: Completed
          startedAt: "2023-05-19T02:58:00Z"
    phase: Running
    podIP: 10.28.4.22
    podIPs:
    - ip: 10.28.4.22
    qosClass: BestEffort
    startTime: "2023-05-19T02:57:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-17T05:50:44Z"
    generateName: nginx-b5579447-
    labels:
      app: nginx
      pod-template-hash: b5579447
    name: nginx-b5579447-wm4vj
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: nginx-b5579447
      uid: 7eb3fdb9-ee3f-485e-a9c7-3ca147cee05e
    resourceVersion: "35641946"
    uid: 6fa2381f-2297-421d-a4c4-a96fffde8309
  spec:
    containers:
    - image: nginx:stable
      imagePullPolicy: IfNotPresent
      name: nginx
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nginx
        name: nginx-conf
        readOnly: true
      - mountPath: /var/www/html
        name: index-html
        readOnly: true
      - mountPath: /etc/nginx-passwd
        name: auth
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-957lm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-57s8
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: nginx.conf
          path: nginx.conf
        name: nginx
      name: nginx-conf
    - configMap:
        defaultMode: 420
        items:
        - key: index.html
          path: index.html
        name: nginx
      name: index-html
    - configMap:
        defaultMode: 420
        items:
        - key: auth
          path: passwd
        name: nginx
      name: auth
    - name: kube-api-access-957lm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:50:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T06:01:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T06:01:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T05:50:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f75341a10f99b036294e2b65235906f9059b1acf2412e1558feab6355cfccb3f
      image: docker.io/library/nginx:stable
      imageID: docker.io/library/nginx@sha256:b1a2c7bcc61be621eae24851a976179bfbc72591e43c1fb340f7497ff72128ff
      lastState:
        terminated:
          containerID: containerd://8bd7699a44d1fc520756f25d16ee70a56516dcae69a195c0d03ac46514aa888c
          exitCode: 1
          finishedAt: "2023-05-17T05:56:16Z"
          reason: Error
          startedAt: "2023-05-17T05:56:16Z"
      name: nginx
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2023-05-17T06:01:18Z"
    hostIP: 10.128.0.43
    phase: Running
    podIP: 10.28.2.4
    podIPs:
    - ip: 10.28.2.4
    qosClass: BestEffort
    startTime: "2023-05-17T05:50:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-19T03:38:29Z"
    generateName: testnet-manager-7d94d8874-
    labels:
      app.kubernetes.io/instance: testnet-manager
      app.kubernetes.io/name: testnet-manager
      pod-template-hash: 7d94d8874
    name: testnet-manager-7d94d8874-h65fx
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: testnet-manager-7d94d8874
      uid: 3fbe4ba2-e063-4926-823d-cffd2919a98c
    resourceVersion: "37014138"
    uid: d93ac981-efa4-4786-a4f3-6f194f729946
  spec:
    containers:
    - args:
      - -m
      - gunicorn
      - -k
      - uvicorn.workers.UvicornWorker
      - main:app
      - --bind=0.0.0.0:5000
      - --timeout=3600
      - --capture-output
      - --enable-stdio-inheritance
      - --workers=4
      command:
      - /app/.venv/bin/python
      env:
      - name: NAMESPACE
        value: rococo
      - name: HEALTHY_MIN_PEER_COUNT
        valueFrom:
          configMapKeyRef:
            key: HEALTHY_MIN_PEER_COUNT
            name: testnet-manager
      - name: LOG_LEVEL
        valueFrom:
          configMapKeyRef:
            key: LOG_LEVEL
            name: testnet-manager
      - name: NODE_HTTP_PATTERN
        valueFrom:
          configMapKeyRef:
            key: NODE_HTTP_PATTERN
            name: testnet-manager
      - name: NODE_WS_PATTERN
        valueFrom:
          configMapKeyRef:
            key: NODE_WS_PATTERN
            name: testnet-manager
      - name: TASKS_CRON_SCHEDULE
        valueFrom:
          configMapKeyRef:
            key: TASKS_CRON_SCHEDULE
            name: testnet-manager
      - name: TESTNET_MANAGER_CONSENSUS
        valueFrom:
          configMapKeyRef:
            key: TESTNET_MANAGER_CONSENSUS
            name: testnet-manager
      - name: WS_ENDPOINT
        valueFrom:
          configMapKeyRef:
            key: WS_ENDPOINT
            name: testnet-manager
      - name: SUDO_SEED
        valueFrom:
          secretKeyRef:
            key: SUDO_SEED
            name: testnet-manager
      - name: VALIDATORS_ROOT_SEED
        valueFrom:
          secretKeyRef:
            key: VALIDATORS_ROOT_SEED
            name: testnet-manager
      image: paritytech/testnet-manager:latest
      imagePullPolicy: IfNotPresent
      name: testnet-manager
      ports:
      - containerPort: 5000
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 128Mi
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mhvs5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-5nql
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: testnet-manager
    serviceAccountName: testnet-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-mhvs5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://92aea69298e9ce38f27883341c51a739f016623e1d39218563bf9e584b00b1d0
      image: docker.io/paritytech/testnet-manager:latest
      imageID: docker.io/paritytech/testnet-manager@sha256:ce58ed9a4274aaff8bfd7226e23dc0a4ea7d548af8478dda3611554cdf8be71a
      lastState: {}
      name: testnet-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-05-19T03:38:31Z"
    hostIP: 10.128.0.44
    phase: Running
    podIP: 10.28.3.48
    podIPs:
    - ip: 10.28.3.48
    qosClass: Burstable
    startTime: "2023-05-19T03:38:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2023-05-19T03:38:33Z"
    generateName: testnet-manager-task-scheduler-6f466447cc-
    labels:
      app.kubernetes.io/instance: testnet-manager-task-scheduler
      app.kubernetes.io/name: testnet-manager-task-scheduler
      pod-template-hash: 6f466447cc
    name: testnet-manager-task-scheduler-6f466447cc-vl77t
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: testnet-manager-task-scheduler-6f466447cc
      uid: 6fca691c-672c-4926-a548-e319232ae54b
    resourceVersion: "37014197"
    uid: bcda44bb-1126-4214-aca1-dbcb490cc49f
  spec:
    containers:
    - args:
      - -m
      - gunicorn
      - -k
      - uvicorn.workers.UvicornWorker
      - task-scheduler:app
      - --bind=0.0.0.0:5000
      - --timeout=3600
      - --capture-output
      - --enable-stdio-inheritance
      - --workers=1
      command:
      - /app/.venv/bin/python
      env:
      - name: NAMESPACE
        value: rococo
      - name: HEALTHY_MIN_PEER_COUNT
        valueFrom:
          configMapKeyRef:
            key: HEALTHY_MIN_PEER_COUNT
            name: testnet-manager
      - name: LOG_LEVEL
        valueFrom:
          configMapKeyRef:
            key: LOG_LEVEL
            name: testnet-manager
      - name: NODE_HTTP_PATTERN
        valueFrom:
          configMapKeyRef:
            key: NODE_HTTP_PATTERN
            name: testnet-manager
      - name: NODE_WS_PATTERN
        valueFrom:
          configMapKeyRef:
            key: NODE_WS_PATTERN
            name: testnet-manager
      - name: TASKS_CRON_SCHEDULE
        valueFrom:
          configMapKeyRef:
            key: TASKS_CRON_SCHEDULE
            name: testnet-manager
      - name: TESTNET_MANAGER_CONSENSUS
        valueFrom:
          configMapKeyRef:
            key: TESTNET_MANAGER_CONSENSUS
            name: testnet-manager
      - name: WS_ENDPOINT
        valueFrom:
          configMapKeyRef:
            key: WS_ENDPOINT
            name: testnet-manager
      - name: SUDO_SEED
        valueFrom:
          secretKeyRef:
            key: SUDO_SEED
            name: testnet-manager
      - name: VALIDATORS_ROOT_SEED
        valueFrom:
          secretKeyRef:
            key: VALIDATORS_ROOT_SEED
            name: testnet-manager
      image: paritytech/testnet-manager:latest
      imagePullPolicy: IfNotPresent
      name: testnet-manager
      ports:
      - containerPort: 5000
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 128Mi
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dkmpk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-nos-k8s-pool-3-6a21c796-5nql
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: testnet-manager
    serviceAccountName: testnet-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-dkmpk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-19T03:38:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2bac0c8252402614909962896e1e679b1e00a289387c73acbadf4c00d81f926b
      image: docker.io/paritytech/testnet-manager:latest
      imageID: docker.io/paritytech/testnet-manager@sha256:ce58ed9a4274aaff8bfd7226e23dc0a4ea7d548af8478dda3611554cdf8be71a
      lastState: {}
      name: testnet-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-05-19T03:38:35Z"
    hostIP: 10.128.0.44
    phase: Running
    podIP: 10.28.3.49
    podIPs:
    - ip: 10.28.3.49
    qosClass: Burstable
    startTime: "2023-05-19T03:38:33Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"run":"chainspec"},"name":"chainspec","namespace":"rococo"},"spec":{"ports":[{"port":80,"protocol":"TCP"}],"selector":{"run":"chainspec"}}}
    creationTimestamp: "2023-05-16T20:06:01Z"
    labels:
      run: chainspec
    name: chainspec
    namespace: rococo
    resourceVersion: "35341179"
    uid: e6f015a1-3a43-4747-bfb1-e38e0165f1d3
  spec:
    clusterIP: 10.32.15.250
    clusterIPs:
    - 10.32.15.250
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      run: chainspec
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-bootnode
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: rococo-local
      release: localrococo-bootnode
      role: authority
      validatorAccount: 5GNJqTPyNqANBkUVMN1LPPrxXnFouWXoe2wNSmmEoLctxiZY
    name: localrococo-bootnode
    namespace: rococo
    resourceVersion: "35341639"
    uid: 1436290e-fc0c-41db-8fa1-1e3dbafa530a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-bootnode
      app.kubernetes.io/name: node
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-bootnode
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: rococo-local
      instance: localrococo-bootnode-0
      node: localrococo-bootnode-0
      release: localrococo-bootnode
      role: authority
      validatorAccount: 5GNJqTPyNqANBkUVMN1LPPrxXnFouWXoe2wNSmmEoLctxiZY
    name: localrococo-bootnode-0
    namespace: rococo
    resourceVersion: "35341632"
    uid: c612a3cc-95f5-4650-94d0-cba4d6492746
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-bootnode
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-bootnode-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-bootnode-bob
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:30Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: rococo-local
      release: localrococo-bootnode-bob
      role: authority
      validatorAccount: 5HpG9w8EBLe5XCrbczpwq5TSXvedjrBGCwqxK1iQ7qUsSWFc
    name: localrococo-bootnode-bob
    namespace: rococo
    resourceVersion: "35341515"
    uid: 35c7367b-a7d5-4798-9104-56095a7bd128
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-bootnode-bob
      app.kubernetes.io/name: node
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-bootnode-bob
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:30Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: rococo-local
      instance: localrococo-bootnode-bob-0
      node: localrococo-bootnode-bob-0
      release: localrococo-bootnode-bob
      role: authority
      validatorAccount: 5HpG9w8EBLe5XCrbczpwq5TSXvedjrBGCwqxK1iQ7qUsSWFc
    name: localrococo-bootnode-bob-0
    namespace: rococo
    resourceVersion: "35341509"
    uid: 6574c585-9af2-48d1-8651-b2f6f6dff528
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-bootnode-bob
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-bootnode-bob-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-moonbase-alice
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T03:38:25Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: moonbase-local
      paraId: "2000"
      release: localrococo-moonbase-alice
      role: collator
      ss58Format: "0"
    name: localrococo-moonbase-alice-node
    namespace: rococo
    resourceVersion: "37013959"
    uid: fc01175b-a6c9-4e6f-9cc6-3e5942680082
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-moonbase-alice
      app.kubernetes.io/name: node
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-moonbase-alice
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T03:38:25Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: moonbase-local
      instance: localrococo-moonbase-alice-node-0
      node: localrococo-moonbase-alice-node-0
      paraId: "2000"
      release: localrococo-moonbase-alice
      role: collator
      ss58Format: "0"
    name: localrococo-moonbase-alice-node-0
    namespace: rococo
    resourceVersion: "37013957"
    uid: 93bd0468-8dd2-44bd-b114-2349e101e59a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-moonbase-alice
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-moonbase-alice-node-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-moonbase-collator
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T03:02:48Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: moonbase-local
      paraId: "2000"
      release: localrococo-moonbase-collator
      role: collator
      ss58Format: "0"
    name: localrococo-moonbase-collator-node
    namespace: rococo
    resourceVersion: "36995128"
    uid: e65af376-3980-4f68-86dd-4f4872a6f870
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-moonbase-collator
      app.kubernetes.io/name: node
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-moonbase-collator
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T03:02:48Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: moonbase-local
      instance: localrococo-moonbase-collator-node-0
      node: localrococo-moonbase-collator-node-0
      paraId: "2000"
      release: localrococo-moonbase-collator
      role: collator
      ss58Format: "0"
    name: localrococo-moonbase-collator-node-0
    namespace: rococo
    resourceVersion: "36995126"
    uid: e27b20be-e4ee-4294-803e-c3d7ebda9408
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-moonbase-collator
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-moonbase-collator-node-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-statemint-alice
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: statemint-local
      collatorAccount: 15oF4uVJwmo4TdGW7VfQxNLavjCXviqxT9S1MgbjMNHr6Sp5
      paraId: "1000"
      release: localrococo-statemint-alice
      role: collator
      ss58Format: "0"
    name: localrococo-statemint-alice-node
    namespace: rococo
    resourceVersion: "35341602"
    uid: f7a2f1f8-a391-452d-8161-71ef34701d8e
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-statemint-alice
      app.kubernetes.io/name: node
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-statemint-alice
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: statemint-local
      collatorAccount: 15oF4uVJwmo4TdGW7VfQxNLavjCXviqxT9S1MgbjMNHr6Sp5
      instance: localrococo-statemint-alice-node-0
      node: localrococo-statemint-alice-node-0
      paraId: "1000"
      release: localrococo-statemint-alice
      role: collator
      ss58Format: "0"
    name: localrococo-statemint-alice-node-0
    namespace: rococo
    resourceVersion: "35341605"
    uid: d864bdab-ef41-4543-b924-0ce3be6269de
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-statemint-alice
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-statemint-alice-node-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-statemint-bob
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: statemint-local
      paraId: "1000"
      release: localrococo-statemint-bob
      role: collator
      ss58Format: "0"
    name: localrococo-statemint-bob-node
    namespace: rococo
    resourceVersion: "35341606"
    uid: 61615859-9226-43ed-8961-c96fbd840c0a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-statemint-bob
      app.kubernetes.io/name: node
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-statemint-bob
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: statemint-local
      instance: localrococo-statemint-bob-node-0
      node: localrococo-statemint-bob-node-0
      paraId: "1000"
      release: localrococo-statemint-bob
      role: collator
      ss58Format: "0"
    name: localrococo-statemint-bob-node-0
    namespace: rococo
    resourceVersion: "35341608"
    uid: 493f2a33-55be-4468-a3e5-cf73087cabea
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-statemint-bob
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-statemint-bob-node-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-tellor-alice
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T22:03:39Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: polkadot
      paraId: "3000"
      release: localrococo-tellor-alice
      role: collator
    name: localrococo-tellor-alice-node
    namespace: rococo
    resourceVersion: "35402133"
    uid: 65fe9543-0f64-49be-a920-2e3a1826a94c
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-tellor-alice
      app.kubernetes.io/name: node
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-tellor-alice
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T22:03:39Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: polkadot
      instance: localrococo-tellor-alice-node-0
      node: localrococo-tellor-alice-node-0
      paraId: "3000"
      release: localrococo-tellor-alice
      role: collator
    name: localrococo-tellor-alice-node-0
    namespace: rococo
    resourceVersion: "35402134"
    uid: 2e68109e-4593-46a2-9da3-f82539f45cc2
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-tellor-alice
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-tellor-alice-node-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-tellor-bob
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T22:03:39Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: polkadot
      paraId: "3000"
      release: localrococo-tellor-bob
      role: collator
    name: localrococo-tellor-bob-node
    namespace: rococo
    resourceVersion: "35402127"
    uid: c171f759-262a-42d6-b832-010c42f4b651
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-tellor-bob
      app.kubernetes.io/name: node
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-tellor-bob
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T22:03:39Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: polkadot
      instance: localrococo-tellor-bob-node-0
      node: localrococo-tellor-bob-node-0
      paraId: "3000"
      release: localrococo-tellor-bob
      role: collator
    name: localrococo-tellor-bob-node-0
    namespace: rococo
    resourceVersion: "35402125"
    uid: 1d7140af-568f-4dab-ad77-e690571e0ecb
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-tellor-bob
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-tellor-bob-node-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-validator-a
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T02:57:47Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: rococo-local
      release: localrococo-validator-a
      role: authority
    name: localrococo-validator-a-node
    namespace: rococo
    resourceVersion: "36992323"
    uid: 685d63ee-2af3-43ab-8a73-ee0838600ae9
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-validator-a
      app.kubernetes.io/name: node
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-validator-a
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T02:57:47Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: rococo-local
      instance: localrococo-validator-a-node-0
      node: localrococo-validator-a-node-0
      release: localrococo-validator-a
      role: authority
    name: localrococo-validator-a-node-0
    namespace: rococo
    resourceVersion: "36992321"
    uid: c0075a92-083c-4b28-ba45-93e9417b0ba9
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-validator-a
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-validator-a-node-0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: localrococo-validator-a
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T02:57:47Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      chain: rococo-local
      instance: localrococo-validator-a-node-1
      node: localrococo-validator-a-node-1
      release: localrococo-validator-a
      role: authority
    name: localrococo-validator-a-node-1
    namespace: rococo
    resourceVersion: "36992324"
    uid: 83fd8683-aa05-4c5e-af13-6a911ebbaec9
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-rpc
      port: 9933
      protocol: TCP
      targetPort: 9933
    - name: websocket-rpc
      port: 9944
      protocol: TCP
      targetPort: 9944
    - name: prometheus
      port: 9615
      protocol: TCP
      targetPort: 9615
    selector:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-validator-a
      app.kubernetes.io/name: node
      statefulset.kubernetes.io/pod-name: localrococo-validator-a-node-1
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: nginx
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-17T05:16:59Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/managed-by: Helm
    name: nginx-frontend
    namespace: rococo
    resourceVersion: "35617278"
    uid: 81bf27ed-199b-4925-9b2b-69d080f1ab63
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.32.13.100
    clusterIPs:
    - 10.32.13.100
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 31002
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: nginx
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 34.70.37.166
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: testnet-manager
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    labels:
      app.kubernetes.io/instance: testnet-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: testnet-manager
      app.kubernetes.io/version: 1.3.0
      helm.sh/chart: testnet-manager-1.2.0
    name: testnet-manager
    namespace: rococo
    resourceVersion: "35341728"
    uid: d49fc69f-37a5-48d8-8954-670d2ed7fad3
  spec:
    clusterIP: 10.32.3.104
    clusterIPs:
    - 10.32.3.104
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: testnet-manager
      app.kubernetes.io/name: testnet-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      cloud.google.com/neg: '{"ingress":true}'
      meta.helm.sh/release-name: testnet-manager
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    labels:
      app.kubernetes.io/instance: testnet-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: testnet-manager
      app.kubernetes.io/version: 1.3.0
      helm.sh/chart: testnet-manager-1.2.0
    name: testnet-manager-task-scheduler
    namespace: rococo
    resourceVersion: "35341731"
    uid: 9a28ccc1-e9d8-4333-b7dc-81003f083e8e
  spec:
    clusterIP: 10.32.7.99
    clusterIPs:
    - 10.32.7.99
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: testnet-manager-task-scheduler
      app.kubernetes.io/name: testnet-manager-task-scheduler
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"chainspec","namespace":"rococo"},"spec":{"replicas":1,"selector":{"matchLabels":{"run":"chainspec"}},"template":{"metadata":{"labels":{"run":"chainspec"}},"spec":{"containers":[{"image":"nginx:stable","imagePullPolicy":"IfNotPresent","name":"chainspec","ports":[{"containerPort":80}],"volumeMounts":[{"mountPath":"/usr/share/nginx/html","name":"dir"}]}],"initContainers":[{"args":["-c","polkadot build-spec --chain rococo-local --disable-default-bootnode --raw  \u003e /dir/rococo-local.json"],"command":["/bin/sh"],"image":"ddorgan/tellor-polkadot:dev","imagePullPolicy":"IfNotPresent","name":"create-chainspec-rococolocal","volumeMounts":[{"mountPath":"/dir","name":"dir"}]},{"args":["-c","set -xe; polkadot-parachain build-spec --chain statemint-local --disable-default-bootnode \u003e /dir/statemint-plain.json; sed 's/\"relay_chain\": \"polkadot-local\"/\"relay_chain\": \"rococo_local_testnet\"/' -i /dir/statemint-plain.json; polkadot-parachain build-spec --chain /dir/statemint-plain.json --disable-default-bootnode --raw  \u003e /dir/statemint.json;\n"],"command":["/bin/sh"],"image":"ddorgan/polkadot-parachain:dev","name":"create-chainspec-statemint","volumeMounts":[{"mountPath":"/dir","name":"dir"}]},{"args":["-c","set -xe; moonbeam build-spec --chain moonbase-local --disable-default-bootnode \u003e /dir/moonbase-dev-plain.json; sed 's/\"relayChain\": \"westend-local\"/\"relayChain\": \"rococo_local_testnet\"/' -i /dir/moonbase-dev-plain.json; sed 's/\"paraId\": 1000/\"paraId\": 2000/' -i /dir/moonbase-dev-plain.json; sed 's/\"parachainId\": 1000/\"parachainId\": 2000/' -i /dir/moonbase-dev-plain.json; moonbeam build-spec --chain /dir/moonbase-dev-plain.json --disable-default-bootnode --raw \u003e /dir/moonbase.json;\n"],"command":["/bin/sh"],"image":"ddorgan/moonbeam:dev","imagePullPolicy":"IfNotPresent","name":"create-chainspec-moonbase","volumeMounts":[{"mountPath":"/dir","name":"dir"}]},{"args":["-c","set -xe; parachain-template-node build-spec  --disable-default-bootnode \u003e /dir/tellor-plain.json; sed 's/\"para_id\": 1000/\"para_id\": 3000/' -i /dir/tellor-plain.json; sed 's/\"relay_chain\": \"rococo-local\"/\"relay_chain\": \"rococo_local_testnet\"/' -i /dir/tellor-plain.json; sed 's/\"parachainId\": 1000/\"parachainId\": 3000/' -i /dir/tellor-plain.json; parachain-template-node build-spec --chain /dir/tellor-plain.json --disable-default-bootnode --raw \u003e /dir/tellor.json;\n"],"command":["/bin/sh"],"image":"ddorgan/tellor-oracle:rust-locked","imagePullPolicy":"IfNotPresent","name":"create-chainspec-tellor","volumeMounts":[{"mountPath":"/dir","name":"dir"}]}],"volumes":[{"emptyDir":{},"name":"dir"}]}}}}
    creationTimestamp: "2023-05-16T20:06:02Z"
    generation: 3
    name: chainspec
    namespace: rococo
    resourceVersion: "35636723"
    uid: 05ef0a30-2ea2-463b-afa0-9d1da7e1003c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        run: chainspec
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          run: chainspec
      spec:
        containers:
        - image: nginx:stable
          imagePullPolicy: IfNotPresent
          name: chainspec
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/share/nginx/html
            name: dir
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - polkadot build-spec --chain rococo-local --disable-default-bootnode --raw  >
            /dir/rococo-local.json
          command:
          - /bin/sh
          image: ddorgan/tellor-polkadot:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-rococolocal
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; polkadot-parachain build-spec --chain statemint-local --disable-default-bootnode > /dir/statemint-plain.json; sed 's/"relay_chain": "polkadot-local"/"relay_chain": "rococo_local_testnet"/' -i /dir/statemint-plain.json; polkadot-parachain build-spec --chain /dir/statemint-plain.json --disable-default-bootnode --raw  > /dir/statemint.json;
          command:
          - /bin/sh
          image: ddorgan/polkadot-parachain:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-statemint
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; moonbeam build-spec --chain moonbase-local --disable-default-bootnode > /dir/moonbase-dev-plain.json; sed 's/"relayChain": "westend-local"/"relayChain": "rococo_local_testnet"/' -i /dir/moonbase-dev-plain.json; sed 's/"paraId": 1000/"paraId": 2000/' -i /dir/moonbase-dev-plain.json; sed 's/"parachainId": 1000/"parachainId": 2000/' -i /dir/moonbase-dev-plain.json; moonbeam build-spec --chain /dir/moonbase-dev-plain.json --disable-default-bootnode --raw > /dir/moonbase.json;
          command:
          - /bin/sh
          image: ddorgan/moonbeam:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-moonbase
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; parachain-template-node build-spec  --disable-default-bootnode > /dir/tellor-plain.json; sed 's/"para_id": 1000/"para_id": 3000/' -i /dir/tellor-plain.json; sed 's/"relay_chain": "rococo-local"/"relay_chain": "rococo_local_testnet"/' -i /dir/tellor-plain.json; sed 's/"parachainId": 1000/"parachainId": 3000/' -i /dir/tellor-plain.json; parachain-template-node build-spec --chain /dir/tellor-plain.json --disable-default-bootnode --raw > /dir/tellor.json;
          command:
          - /bin/sh
          image: ddorgan/tellor-oracle:rust-locked
          imagePullPolicy: IfNotPresent
          name: create-chainspec-tellor
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-16T20:06:02Z"
      lastUpdateTime: "2023-05-16T22:03:34Z"
      message: ReplicaSet "chainspec-85d9949cc8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2023-05-17T05:51:44Z"
      lastUpdateTime: "2023-05-17T05:51:44Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: nginx
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-17T05:16:59Z"
    generation: 1
    labels:
      app: nginx
      app.kubernetes.io/managed-by: Helm
    name: nginx
    namespace: rococo
    resourceVersion: "35641950"
    uid: ff17f1cc-15d6-49c1-9047-a5c520d917de
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx
      spec:
        containers:
        - image: nginx:stable
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx
            name: nginx-conf
            readOnly: true
          - mountPath: /var/www/html
            name: index-html
            readOnly: true
          - mountPath: /etc/nginx-passwd
            name: auth
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: nginx.conf
              path: nginx.conf
            name: nginx
          name: nginx-conf
        - configMap:
            defaultMode: 420
            items:
            - key: index.html
              path: index.html
            name: nginx
          name: index-html
        - configMap:
            defaultMode: 420
            items:
            - key: auth
              path: passwd
            name: nginx
          name: auth
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-17T05:16:59Z"
      lastUpdateTime: "2023-05-17T05:20:01Z"
      message: ReplicaSet "nginx-b5579447" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2023-05-17T06:01:18Z"
      lastUpdateTime: "2023-05-17T06:01:18Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: testnet-manager
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:33Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: testnet-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: testnet-manager
      app.kubernetes.io/version: 1.3.0
      helm.sh/chart: testnet-manager-1.2.0
    name: testnet-manager
    namespace: rococo
    resourceVersion: "37014142"
    uid: b3eb9b98-f6f2-447b-bff4-1656a4bde82a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: testnet-manager
        app.kubernetes.io/name: testnet-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: testnet-manager
          app.kubernetes.io/name: testnet-manager
      spec:
        containers:
        - args:
          - -m
          - gunicorn
          - -k
          - uvicorn.workers.UvicornWorker
          - main:app
          - --bind=0.0.0.0:5000
          - --timeout=3600
          - --capture-output
          - --enable-stdio-inheritance
          - --workers=4
          command:
          - /app/.venv/bin/python
          env:
          - name: NAMESPACE
            value: rococo
          - name: HEALTHY_MIN_PEER_COUNT
            valueFrom:
              configMapKeyRef:
                key: HEALTHY_MIN_PEER_COUNT
                name: testnet-manager
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: LOG_LEVEL
                name: testnet-manager
          - name: NODE_HTTP_PATTERN
            valueFrom:
              configMapKeyRef:
                key: NODE_HTTP_PATTERN
                name: testnet-manager
          - name: NODE_WS_PATTERN
            valueFrom:
              configMapKeyRef:
                key: NODE_WS_PATTERN
                name: testnet-manager
          - name: TASKS_CRON_SCHEDULE
            valueFrom:
              configMapKeyRef:
                key: TASKS_CRON_SCHEDULE
                name: testnet-manager
          - name: TESTNET_MANAGER_CONSENSUS
            valueFrom:
              configMapKeyRef:
                key: TESTNET_MANAGER_CONSENSUS
                name: testnet-manager
          - name: WS_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: WS_ENDPOINT
                name: testnet-manager
          - name: SUDO_SEED
            valueFrom:
              secretKeyRef:
                key: SUDO_SEED
                name: testnet-manager
          - name: VALIDATORS_ROOT_SEED
            valueFrom:
              secretKeyRef:
                key: VALIDATORS_ROOT_SEED
                name: testnet-manager
          image: paritytech/testnet-manager:latest
          imagePullPolicy: IfNotPresent
          name: testnet-manager
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 128Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: testnet-manager
        serviceAccountName: testnet-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-16T20:06:33Z"
      lastUpdateTime: "2023-05-16T20:06:56Z"
      message: ReplicaSet "testnet-manager-7d94d8874" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2023-05-19T03:38:40Z"
      lastUpdateTime: "2023-05-19T03:38:40Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: testnet-manager
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:33Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: testnet-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: testnet-manager
      app.kubernetes.io/version: 1.3.0
      helm.sh/chart: testnet-manager-1.2.0
    name: testnet-manager-task-scheduler
    namespace: rococo
    resourceVersion: "37014201"
    uid: 27c5dc6d-1a6e-4eef-aaed-1cafc25ccf2f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: testnet-manager-task-scheduler
        app.kubernetes.io/name: testnet-manager-task-scheduler
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: testnet-manager-task-scheduler
          app.kubernetes.io/name: testnet-manager-task-scheduler
      spec:
        containers:
        - args:
          - -m
          - gunicorn
          - -k
          - uvicorn.workers.UvicornWorker
          - task-scheduler:app
          - --bind=0.0.0.0:5000
          - --timeout=3600
          - --capture-output
          - --enable-stdio-inheritance
          - --workers=1
          command:
          - /app/.venv/bin/python
          env:
          - name: NAMESPACE
            value: rococo
          - name: HEALTHY_MIN_PEER_COUNT
            valueFrom:
              configMapKeyRef:
                key: HEALTHY_MIN_PEER_COUNT
                name: testnet-manager
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: LOG_LEVEL
                name: testnet-manager
          - name: NODE_HTTP_PATTERN
            valueFrom:
              configMapKeyRef:
                key: NODE_HTTP_PATTERN
                name: testnet-manager
          - name: NODE_WS_PATTERN
            valueFrom:
              configMapKeyRef:
                key: NODE_WS_PATTERN
                name: testnet-manager
          - name: TASKS_CRON_SCHEDULE
            valueFrom:
              configMapKeyRef:
                key: TASKS_CRON_SCHEDULE
                name: testnet-manager
          - name: TESTNET_MANAGER_CONSENSUS
            valueFrom:
              configMapKeyRef:
                key: TESTNET_MANAGER_CONSENSUS
                name: testnet-manager
          - name: WS_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: WS_ENDPOINT
                name: testnet-manager
          - name: SUDO_SEED
            valueFrom:
              secretKeyRef:
                key: SUDO_SEED
                name: testnet-manager
          - name: VALIDATORS_ROOT_SEED
            valueFrom:
              secretKeyRef:
                key: VALIDATORS_ROOT_SEED
                name: testnet-manager
          image: paritytech/testnet-manager:latest
          imagePullPolicy: IfNotPresent
          name: testnet-manager
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 128Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: testnet-manager
        serviceAccountName: testnet-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-16T20:06:33Z"
      lastUpdateTime: "2023-05-16T20:06:56Z"
      message: ReplicaSet "testnet-manager-task-scheduler-6f466447cc" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2023-05-19T03:38:44Z"
      lastUpdateTime: "2023-05-19T03:38:44Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2023-05-16T21:08:14Z"
    generation: 2
    labels:
      pod-template-hash: 6dd7d8b79
      run: chainspec
    name: chainspec-6dd7d8b79
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: chainspec
      uid: 05ef0a30-2ea2-463b-afa0-9d1da7e1003c
    resourceVersion: "35402056"
    uid: 6226d191-8b4d-407b-9ec8-1bc600372500
  spec:
    replicas: 0
    selector:
      matchLabels:
        pod-template-hash: 6dd7d8b79
        run: chainspec
    template:
      metadata:
        creationTimestamp: null
        labels:
          pod-template-hash: 6dd7d8b79
          run: chainspec
      spec:
        containers:
        - image: nginx:stable
          imagePullPolicy: IfNotPresent
          name: chainspec
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/share/nginx/html
            name: dir
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - polkadot build-spec --chain rococo-local --disable-default-bootnode --raw  >
            /dir/rococo-local.json
          command:
          - /bin/sh
          image: ddorgan/tellor-polkadot:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-rococolocal
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; polkadot-parachain build-spec --chain statemint-local --disable-default-bootnode > /dir/statemint-plain.json; sed 's/"relay_chain": "polkadot-local"/"relay_chain": "rococo_local_testnet"/' -i /dir/statemint-plain.json; polkadot-parachain build-spec --chain /dir/statemint-plain.json --disable-default-bootnode --raw  > /dir/statemint.json;
          command:
          - /bin/sh
          image: ddorgan/polkadot-parachain:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-statemint
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; moonbeam build-spec --chain moonbase-local --disable-default-bootnode > /dir/moonbase-dev-plain.json; sed 's/"relayChain": "westend-local"/"relayChain": "rococo_local_testnet"/' -i /dir/moonbase-dev-plain.json; sed 's/"paraId": 1000/"paraId": 2000/' -i /dir/moonbase-dev-plain.json; sed 's/"parachainId": 1000/"parachainId": 2000/' -i /dir/moonbase-dev-plain.json; moonbeam build-spec --chain /dir/moonbase-dev-plain.json --disable-default-bootnode --raw > /dir/moonbase.json;
          command:
          - /bin/sh
          image: ddorgan/moonbeam:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-moonbase
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; parachain-template-node build-spec --chain local  --disable-default-bootnode > /dir/tellor-plain.json; sed 's/"para_id": 1000/"para_id": 3000/' -i /dir/tellor-plain.json; sed 's/"parachainId": 1000/"parachainId": 3000/' -i /dir/tellor-plain.json; parachain-template-node build-spec --chain /dir/tellor-plain.json --disable-default-bootnode --raw > /dir/tellor.json;
          command:
          - /bin/sh
          image: ddorgan/tellor-oracle:0.1.0-d5b7b102f5d-2
          imagePullPolicy: IfNotPresent
          name: create-chainspec-tellor
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2023-05-16T20:06:02Z"
    generation: 2
    labels:
      pod-template-hash: 78f6fc65c7
      run: chainspec
    name: chainspec-78f6fc65c7
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: chainspec
      uid: 05ef0a30-2ea2-463b-afa0-9d1da7e1003c
    resourceVersion: "35373221"
    uid: 6fa2084f-6c92-4d2f-bd69-41140fdb4d71
  spec:
    replicas: 0
    selector:
      matchLabels:
        pod-template-hash: 78f6fc65c7
        run: chainspec
    template:
      metadata:
        creationTimestamp: null
        labels:
          pod-template-hash: 78f6fc65c7
          run: chainspec
      spec:
        containers:
        - image: nginx:stable
          imagePullPolicy: IfNotPresent
          name: chainspec
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/share/nginx/html
            name: dir
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - polkadot build-spec --chain rococo-local --disable-default-bootnode --raw  >
            /dir/rococo-local.json
          command:
          - /bin/sh
          image: ddorgan/tellor-polkadot:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-rococolocal
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; polkadot-parachain build-spec --chain statemint-local --disable-default-bootnode > /dir/statemint-plain.json; sed 's/"relay_chain": "polkadot-local"/"relay_chain": "rococo_local_testnet"/' -i /dir/statemint-plain.json; polkadot-parachain build-spec --chain /dir/statemint-plain.json --disable-default-bootnode --raw  > /dir/statemint.json;
          command:
          - /bin/sh
          image: ddorgan/polkadot-parachain:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-statemint
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; moonbeam build-spec --chain moonbase-local --disable-default-bootnode > /dir/moonbase-dev-plain.json; sed 's/"relayChain": "westend-local"/"relayChain": "rococo_local_testnet"/' -i /dir/moonbase-dev-plain.json; sed 's/"paraId": 1000/"paraId": 2000/' -i /dir/moonbase-dev-plain.json; sed 's/"parachainId": 1000/"parachainId": 2000/' -i /dir/moonbase-dev-plain.json; moonbeam build-spec --chain /dir/moonbase-dev-plain.json --disable-default-bootnode --raw > /dir/moonbase.json;
          command:
          - /bin/sh
          image: ddorgan/moonbeam:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-moonbase
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; parachain-template-node build-spec --chain local  --disable-default-bootnode > /dir/tellor-plain.json; sed 's/"para_id": 1000/"para_id": 3000/' -i /dir/tellor-plain.json; sed 's/"parachainId": 1000/"parachainId": 3000/' -i /dir/tellor-plain.json; parachain-template-node build-spec --chain /dir/tellor-plain.json --disable-default-bootnode --raw > /dir/tellor.json;
          command:
          - /bin/sh
          image: ddorgan/tellor-oracle:d5b7b102f5d-2
          imagePullPolicy: IfNotPresent
          name: create-chainspec-tellor
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2023-05-16T22:03:28Z"
    generation: 1
    labels:
      pod-template-hash: 85d9949cc8
      run: chainspec
    name: chainspec-85d9949cc8
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: chainspec
      uid: 05ef0a30-2ea2-463b-afa0-9d1da7e1003c
    resourceVersion: "35636722"
    uid: 2694a96b-93b2-479d-a9b1-644db1559b30
  spec:
    replicas: 1
    selector:
      matchLabels:
        pod-template-hash: 85d9949cc8
        run: chainspec
    template:
      metadata:
        creationTimestamp: null
        labels:
          pod-template-hash: 85d9949cc8
          run: chainspec
      spec:
        containers:
        - image: nginx:stable
          imagePullPolicy: IfNotPresent
          name: chainspec
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/share/nginx/html
            name: dir
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - polkadot build-spec --chain rococo-local --disable-default-bootnode --raw  >
            /dir/rococo-local.json
          command:
          - /bin/sh
          image: ddorgan/tellor-polkadot:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-rococolocal
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; polkadot-parachain build-spec --chain statemint-local --disable-default-bootnode > /dir/statemint-plain.json; sed 's/"relay_chain": "polkadot-local"/"relay_chain": "rococo_local_testnet"/' -i /dir/statemint-plain.json; polkadot-parachain build-spec --chain /dir/statemint-plain.json --disable-default-bootnode --raw  > /dir/statemint.json;
          command:
          - /bin/sh
          image: ddorgan/polkadot-parachain:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-statemint
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; moonbeam build-spec --chain moonbase-local --disable-default-bootnode > /dir/moonbase-dev-plain.json; sed 's/"relayChain": "westend-local"/"relayChain": "rococo_local_testnet"/' -i /dir/moonbase-dev-plain.json; sed 's/"paraId": 1000/"paraId": 2000/' -i /dir/moonbase-dev-plain.json; sed 's/"parachainId": 1000/"parachainId": 2000/' -i /dir/moonbase-dev-plain.json; moonbeam build-spec --chain /dir/moonbase-dev-plain.json --disable-default-bootnode --raw > /dir/moonbase.json;
          command:
          - /bin/sh
          image: ddorgan/moonbeam:dev
          imagePullPolicy: IfNotPresent
          name: create-chainspec-moonbase
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        - args:
          - -c
          - |
            set -xe; parachain-template-node build-spec  --disable-default-bootnode > /dir/tellor-plain.json; sed 's/"para_id": 1000/"para_id": 3000/' -i /dir/tellor-plain.json; sed 's/"relay_chain": "rococo-local"/"relay_chain": "rococo_local_testnet"/' -i /dir/tellor-plain.json; sed 's/"parachainId": 1000/"parachainId": 3000/' -i /dir/tellor-plain.json; parachain-template-node build-spec --chain /dir/tellor-plain.json --disable-default-bootnode --raw > /dir/tellor.json;
          command:
          - /bin/sh
          image: ddorgan/tellor-oracle:rust-locked
          imagePullPolicy: IfNotPresent
          name: create-chainspec-tellor
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dir
            name: dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: nginx
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-17T05:16:59Z"
    generation: 1
    labels:
      app: nginx
      pod-template-hash: b5579447
    name: nginx-b5579447
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: nginx
      uid: ff17f1cc-15d6-49c1-9047-a5c520d917de
    resourceVersion: "35641949"
    uid: 7eb3fdb9-ee3f-485e-a9c7-3ca147cee05e
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx
        pod-template-hash: b5579447
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx
          pod-template-hash: b5579447
      spec:
        containers:
        - image: nginx:stable
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx
            name: nginx-conf
            readOnly: true
          - mountPath: /var/www/html
            name: index-html
            readOnly: true
          - mountPath: /etc/nginx-passwd
            name: auth
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: nginx.conf
              path: nginx.conf
            name: nginx
          name: nginx-conf
        - configMap:
            defaultMode: 420
            items:
            - key: index.html
              path: index.html
            name: nginx
          name: index-html
        - configMap:
            defaultMode: 420
            items:
            - key: auth
              path: passwd
            name: nginx
          name: auth
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: testnet-manager
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:33Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: testnet-manager
      app.kubernetes.io/name: testnet-manager
      pod-template-hash: 7d94d8874
    name: testnet-manager-7d94d8874
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: testnet-manager
      uid: b3eb9b98-f6f2-447b-bff4-1656a4bde82a
    resourceVersion: "37014141"
    uid: 3fbe4ba2-e063-4926-823d-cffd2919a98c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: testnet-manager
        app.kubernetes.io/name: testnet-manager
        pod-template-hash: 7d94d8874
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: testnet-manager
          app.kubernetes.io/name: testnet-manager
          pod-template-hash: 7d94d8874
      spec:
        containers:
        - args:
          - -m
          - gunicorn
          - -k
          - uvicorn.workers.UvicornWorker
          - main:app
          - --bind=0.0.0.0:5000
          - --timeout=3600
          - --capture-output
          - --enable-stdio-inheritance
          - --workers=4
          command:
          - /app/.venv/bin/python
          env:
          - name: NAMESPACE
            value: rococo
          - name: HEALTHY_MIN_PEER_COUNT
            valueFrom:
              configMapKeyRef:
                key: HEALTHY_MIN_PEER_COUNT
                name: testnet-manager
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: LOG_LEVEL
                name: testnet-manager
          - name: NODE_HTTP_PATTERN
            valueFrom:
              configMapKeyRef:
                key: NODE_HTTP_PATTERN
                name: testnet-manager
          - name: NODE_WS_PATTERN
            valueFrom:
              configMapKeyRef:
                key: NODE_WS_PATTERN
                name: testnet-manager
          - name: TASKS_CRON_SCHEDULE
            valueFrom:
              configMapKeyRef:
                key: TASKS_CRON_SCHEDULE
                name: testnet-manager
          - name: TESTNET_MANAGER_CONSENSUS
            valueFrom:
              configMapKeyRef:
                key: TESTNET_MANAGER_CONSENSUS
                name: testnet-manager
          - name: WS_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: WS_ENDPOINT
                name: testnet-manager
          - name: SUDO_SEED
            valueFrom:
              secretKeyRef:
                key: SUDO_SEED
                name: testnet-manager
          - name: VALIDATORS_ROOT_SEED
            valueFrom:
              secretKeyRef:
                key: VALIDATORS_ROOT_SEED
                name: testnet-manager
          image: paritytech/testnet-manager:latest
          imagePullPolicy: IfNotPresent
          name: testnet-manager
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 128Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: testnet-manager
        serviceAccountName: testnet-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: testnet-manager
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:33Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: testnet-manager-task-scheduler
      app.kubernetes.io/name: testnet-manager-task-scheduler
      pod-template-hash: 6f466447cc
    name: testnet-manager-task-scheduler-6f466447cc
    namespace: rococo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: testnet-manager-task-scheduler
      uid: 27c5dc6d-1a6e-4eef-aaed-1cafc25ccf2f
    resourceVersion: "37014200"
    uid: 6fca691c-672c-4926-a548-e319232ae54b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: testnet-manager-task-scheduler
        app.kubernetes.io/name: testnet-manager-task-scheduler
        pod-template-hash: 6f466447cc
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: testnet-manager-task-scheduler
          app.kubernetes.io/name: testnet-manager-task-scheduler
          pod-template-hash: 6f466447cc
      spec:
        containers:
        - args:
          - -m
          - gunicorn
          - -k
          - uvicorn.workers.UvicornWorker
          - task-scheduler:app
          - --bind=0.0.0.0:5000
          - --timeout=3600
          - --capture-output
          - --enable-stdio-inheritance
          - --workers=1
          command:
          - /app/.venv/bin/python
          env:
          - name: NAMESPACE
            value: rococo
          - name: HEALTHY_MIN_PEER_COUNT
            valueFrom:
              configMapKeyRef:
                key: HEALTHY_MIN_PEER_COUNT
                name: testnet-manager
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: LOG_LEVEL
                name: testnet-manager
          - name: NODE_HTTP_PATTERN
            valueFrom:
              configMapKeyRef:
                key: NODE_HTTP_PATTERN
                name: testnet-manager
          - name: NODE_WS_PATTERN
            valueFrom:
              configMapKeyRef:
                key: NODE_WS_PATTERN
                name: testnet-manager
          - name: TASKS_CRON_SCHEDULE
            valueFrom:
              configMapKeyRef:
                key: TASKS_CRON_SCHEDULE
                name: testnet-manager
          - name: TESTNET_MANAGER_CONSENSUS
            valueFrom:
              configMapKeyRef:
                key: TESTNET_MANAGER_CONSENSUS
                name: testnet-manager
          - name: WS_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: WS_ENDPOINT
                name: testnet-manager
          - name: SUDO_SEED
            valueFrom:
              secretKeyRef:
                key: SUDO_SEED
                name: testnet-manager
          - name: VALIDATORS_ROOT_SEED
            valueFrom:
              secretKeyRef:
                key: VALIDATORS_ROOT_SEED
                name: testnet-manager
          image: paritytech/testnet-manager:latest
          imagePullPolicy: IfNotPresent
          name: testnet-manager
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 128Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: testnet-manager
        serviceAccountName: testnet-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: localrococo-bootnode
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    generation: 1
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-bootnode
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: rococo-local
      database: rocksdb
      helm.sh/chart: node-4.6.1
      pruning: archive
      release: localrococo-bootnode
      role: authority
      validatorAccount: 5GNJqTPyNqANBkUVMN1LPPrxXnFouWXoe2wNSmmEoLctxiZY
    name: localrococo-bootnode
    namespace: rococo
    resourceVersion: "36951498"
    uid: 022c2fbf-493a-40aa-93ba-4d74193152f0
  spec:
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: substrate-node
        app.kubernetes.io/instance: localrococo-bootnode
        app.kubernetes.io/name: node
    serviceName: localrococo-bootnode
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: substrate-node
          app.kubernetes.io/instance: localrococo-bootnode
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: node
          app.kubernetes.io/version: dev
          chain: rococo-local
          database: rocksdb
          helm.sh/chart: node-4.6.1
          pruning: archive
          release: localrococo-bootnode
          role: authority
          validatorAccount: 5GNJqTPyNqANBkUVMN1LPPrxXnFouWXoe2wNSmmEoLctxiZY
      spec:
        containers:
        - args:
          - -c
          - |
            set -eu
            POD_INDEX="${HOSTNAME##*-}"
            RELAY_CHAIN_P2P_PORT="30333"
            echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
            exec polkadot \
              --name=${POD_NAME} \
              --base-path=/chain-data \
              --keystore-path=/keystore \
              --chain=/chain-data/chainspec.json \
              --validator \
              --database=rocksdb \
              --pruning=archive \
              --prometheus-external \
              --prometheus-port 9615 \
              --unsafe-rpc-external \
              --unsafe-ws-external \
              --rpc-cors=all \
              --rpc-methods=unsafe \
              --node-key $(cat /custom-node-key/custom-node-key) \
              --alice \
              --ws-max-connections=5000 \
              --listen-addr=/ip4/0.0.0.0/tcp/30333 \
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: rococo-local
          - name: NODE_NAME
            value: $(POD_NAME)
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ddorgan/tellor-polkadot:dev
          imagePullPolicy: Always
          name: rococo-local
          ports:
          - containerPort: 9933
            name: http-rpc
            protocol: TCP
          - containerPort: 9944
            name: websocket-rpc
            protocol: TCP
          - containerPort: 9615
            name: prometheus
            protocol: TCP
          - containerPort: 30333
            name: p2p
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http-rpc
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /keystore
            name: chain-keystore
          - mountPath: /custom-node-key/
            name: custom-node-key
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - |
            set -eu -o pipefail -x
              wget -O /chain-data/chainspec.json http://chainspec.rococo/rococo-local.json
          command:
          - /bin/sh
          image: paritytech/lz4:latest
          imagePullPolicy: Always
          name: download-chainspec
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        - args:
          - -c
          - |
            set -eu -x
            POD_INDEX="${HOSTNAME##*-}"
          command:
          - /bin/sh
          image: paritytech/kubetools-kubectl:latest
          imagePullPolicy: Always
          name: retrieve-service-info
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        - args:
          - /keystore
          image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
          imagePullPolicy: IfNotPresent
          name: dump-session-keys
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /keystore
            name: chain-keystore
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: localrococo-bootnode
        serviceAccountName: localrococo-bootnode
        terminationGracePeriodSeconds: 60
        volumes:
        - name: custom-node-key
          secret:
            defaultMode: 420
            secretName: localrococo-bootnode-custom-node-key
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: chain-keystore
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: chain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: localrococo-bootnode-5cf6df7767
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: localrococo-bootnode-5cf6df7767
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: localrococo-bootnode-bob
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:31Z"
    generation: 1
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-bootnode-bob
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: rococo-local
      database: rocksdb
      helm.sh/chart: node-4.6.1
      pruning: archive
      release: localrococo-bootnode-bob
      role: authority
      validatorAccount: 5HpG9w8EBLe5XCrbczpwq5TSXvedjrBGCwqxK1iQ7qUsSWFc
    name: localrococo-bootnode-bob
    namespace: rococo
    resourceVersion: "36951503"
    uid: d77d5cf2-b726-46ef-81c3-fd7e66465ec6
  spec:
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: substrate-node
        app.kubernetes.io/instance: localrococo-bootnode-bob
        app.kubernetes.io/name: node
    serviceName: localrococo-bootnode-bob
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: substrate-node
          app.kubernetes.io/instance: localrococo-bootnode-bob
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: node
          app.kubernetes.io/version: dev
          chain: rococo-local
          database: rocksdb
          helm.sh/chart: node-4.6.1
          pruning: archive
          release: localrococo-bootnode-bob
          role: authority
          validatorAccount: 5HpG9w8EBLe5XCrbczpwq5TSXvedjrBGCwqxK1iQ7qUsSWFc
      spec:
        containers:
        - args:
          - -c
          - |
            set -eu
            POD_INDEX="${HOSTNAME##*-}"
            RELAY_CHAIN_P2P_PORT="30333"
            echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
            exec polkadot \
              --name=${POD_NAME} \
              --base-path=/chain-data \
              --keystore-path=/keystore \
              --chain=/chain-data/chainspec.json \
              --validator \
              --database=rocksdb \
              --pruning=archive \
              --prometheus-external \
              --prometheus-port 9615 \
              --unsafe-rpc-external \
              --unsafe-ws-external \
              --rpc-cors=all \
              --rpc-methods=unsafe \
              --bob \
              --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
              --listen-addr=/ip4/0.0.0.0/tcp/30333 \
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: rococo-local
          - name: NODE_NAME
            value: $(POD_NAME)
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ddorgan/tellor-polkadot:dev
          imagePullPolicy: Always
          name: rococo-local
          ports:
          - containerPort: 9933
            name: http-rpc
            protocol: TCP
          - containerPort: 9944
            name: websocket-rpc
            protocol: TCP
          - containerPort: 9615
            name: prometheus
            protocol: TCP
          - containerPort: 30333
            name: p2p
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http-rpc
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /keystore
            name: chain-keystore
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - |
            set -eu -o pipefail -x
              wget -O /chain-data/chainspec.json http://chainspec.rococo/rococo-local.json
          command:
          - /bin/sh
          image: paritytech/lz4:latest
          imagePullPolicy: Always
          name: download-chainspec
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        - args:
          - -c
          - |
            set -eu -x
            POD_INDEX="${HOSTNAME##*-}"
          command:
          - /bin/sh
          image: paritytech/kubetools-kubectl:latest
          imagePullPolicy: Always
          name: retrieve-service-info
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        - args:
          - /keystore
          image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
          imagePullPolicy: IfNotPresent
          name: dump-session-keys
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /keystore
            name: chain-keystore
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: localrococo-bootnode-bob
        serviceAccountName: localrococo-bootnode-bob
        terminationGracePeriodSeconds: 60
        volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: chain-keystore
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: chain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: localrococo-bootnode-bob-67bc7894b6
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: localrococo-bootnode-bob-67bc7894b6
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: localrococo-moonbase-alice
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T03:38:25Z"
    generation: 1
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-moonbase-alice
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: moonbase-local
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "2000"
      release: localrococo-moonbase-alice
      role: collator
      ss58Format: "0"
    name: localrococo-moonbase-alice-node
    namespace: rococo
    resourceVersion: "37015899"
    uid: 98ada0e1-2187-4b32-b841-51b92571d191
  spec:
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: substrate-node
        app.kubernetes.io/instance: localrococo-moonbase-alice
        app.kubernetes.io/name: node
    serviceName: localrococo-moonbase-alice-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: substrate-node
          app.kubernetes.io/instance: localrococo-moonbase-alice
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: node
          app.kubernetes.io/version: dev
          chain: moonbase-local
          database: rocksdb
          helm.sh/chart: node-4.6.1
          paraId: "2000"
          release: localrococo-moonbase-alice
          role: collator
          ss58Format: "0"
      spec:
        containers:
        - args:
          - -c
          - |
            set -eu
            POD_INDEX="${HOSTNAME##*-}"
            RELAY_CHAIN_P2P_PORT="30333"
            echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
            PARA_CHAIN_P2P_PORT="30334"
            echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
            exec /usr/local/bin/moonbeam \
              --name=${POD_NAME} \
              --base-path=/chain-data \
              --keystore-path=/keystore \
              --chain=/chain-data/chainspec.json \
              --database=rocksdb \
              --collator \
              --prometheus-external \
              --prometheus-port 9615 \
              --unsafe-rpc-external \
              --unsafe-ws-external \
              --rpc-cors=all \
              --rpc-methods=unsafe \
              --listen-addr=/ip4/0.0.0.0/tcp/30334 \
              --node-key $(cat /custom-node-key/custom-node-key) \
              --alice \
              --ws-max-connections=5000 \
              -- \
              --name=${POD_NAME} \
              --base-path=/relaychain-data \
              --keystore-path=/relaychain-keystore \
              --database=rocksdb \
              --chain=/relaychain-data/relay_chain_chainspec.json \
              --bootnodes /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
              --listen-addr=/ip4/0.0.0.0/tcp/30333 \
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: moonbase-local
          - name: NODE_NAME
            value: $(POD_NAME)
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ddorgan/moonbeam:dev
          imagePullPolicy: Always
          name: moonbase-local
          ports:
          - containerPort: 9933
            name: http-rpc
            protocol: TCP
          - containerPort: 9944
            name: websocket-rpc
            protocol: TCP
          - containerPort: 9615
            name: prometheus
            protocol: TCP
          - containerPort: 30333
            name: p2p
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http-rpc
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /keystore
            name: chain-keystore
          - mountPath: /relaychain-data
            name: relaychain-data
          - mountPath: /relaychain-keystore
            name: relaychain-keystore
          - mountPath: /custom-node-key/
            name: custom-node-key
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - |
            set -eu -o pipefail -x
              wget -O /chain-data/chainspec.json http://chainspec.rococo/moonbase.json
              wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
          command:
          - /bin/sh
          image: paritytech/lz4:latest
          imagePullPolicy: Always
          name: download-chainspec
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /relaychain-data
            name: relaychain-data
        - args:
          - -c
          - |
            set -eu -x
            POD_INDEX="${HOSTNAME##*-}"
          command:
          - /bin/sh
          image: paritytech/kubetools-kubectl:latest
          imagePullPolicy: Always
          name: retrieve-service-info
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: localrococo-moonbase-alice-node
        serviceAccountName: localrococo-moonbase-alice-node
        terminationGracePeriodSeconds: 60
        volumes:
        - name: custom-node-key
          secret:
            defaultMode: 420
            secretName: localrococo-moonbase-alice-node-custom-node-key
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: chain-keystore
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: relaychain-keystore
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: chain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: relaychain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: localrococo-moonbase-alice-node-6999b95f54
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: localrococo-moonbase-alice-node-6999b95f54
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: localrococo-moonbase-collator
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T03:02:48Z"
    generation: 2
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-moonbase-collator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: moonbase-local
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "2000"
      pruning: archive
      release: localrococo-moonbase-collator
      role: collator
      ss58Format: "0"
    name: localrococo-moonbase-collator-node
    namespace: rococo
    resourceVersion: "36997633"
    uid: 5fb8ca7b-8396-40b3-8020-f1ec31d503a3
  spec:
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: substrate-node
        app.kubernetes.io/instance: localrococo-moonbase-collator
        app.kubernetes.io/name: node
    serviceName: localrococo-moonbase-collator-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: substrate-node
          app.kubernetes.io/instance: localrococo-moonbase-collator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: node
          app.kubernetes.io/version: dev
          chain: moonbase-local
          database: rocksdb
          helm.sh/chart: node-4.6.1
          paraId: "2000"
          pruning: archive
          release: localrococo-moonbase-collator
          role: collator
          ss58Format: "0"
      spec:
        containers:
        - args:
          - -c
          - |
            set -eu
            POD_INDEX="${HOSTNAME##*-}"
            RELAY_CHAIN_P2P_PORT="30333"
            echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
            PARA_CHAIN_P2P_PORT="30334"
            echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
            exec /usr/local/bin/moonbeam \
              --name=${POD_NAME} \
              --base-path=/chain-data \
              --keystore-path=/keystore \
              --chain=/chain-data/chainspec.json \
              --database=rocksdb \
              --pruning=archive \
              --collator \
              --prometheus-external \
              --prometheus-port 9615 \
              --unsafe-rpc-external \
              --unsafe-ws-external \
              --rpc-cors=all \
              --rpc-methods=unsafe \
              --listen-addr=/ip4/0.0.0.0/tcp/30334 \
              --bootnodes  /dns4/localrococo-moonbase-alice-node-0/tcp/30334/p2p/12D3KooWCGmHA8TkwvxhCXhQgeragffknJL4qN1YQh4AwsE9EL4C \
              -- \
              --name=${POD_NAME} \
              --base-path=/relaychain-data \
              --keystore-path=/relaychain-keystore \
              --database=rocksdb \
              --pruning=1000 \
              --chain=/relaychain-data/relay_chain_chainspec.json \
              --bootnodes /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
              --listen-addr=/ip4/0.0.0.0/tcp/30333 \
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: moonbase-local
          - name: NODE_NAME
            value: $(POD_NAME)
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ddorgan/moonbeam:dev
          imagePullPolicy: Always
          name: moonbase-local
          ports:
          - containerPort: 9933
            name: http-rpc
            protocol: TCP
          - containerPort: 9944
            name: websocket-rpc
            protocol: TCP
          - containerPort: 9615
            name: prometheus
            protocol: TCP
          - containerPort: 30333
            name: p2p
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http-rpc
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /keystore
            name: chain-keystore
          - mountPath: /relaychain-data
            name: relaychain-data
          - mountPath: /relaychain-keystore
            name: relaychain-keystore
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - |
            set -eu -o pipefail -x
              wget -O /chain-data/chainspec.json http://chainspec.rococo/moonbase.json
              wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
          command:
          - /bin/sh
          image: paritytech/lz4:latest
          imagePullPolicy: Always
          name: download-chainspec
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /relaychain-data
            name: relaychain-data
        - args:
          - -c
          - |
            set -eu -x
            POD_INDEX="${HOSTNAME##*-}"
          command:
          - /bin/sh
          image: paritytech/kubetools-kubectl:latest
          imagePullPolicy: Always
          name: retrieve-service-info
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: localrococo-moonbase-collator-node
        serviceAccountName: localrococo-moonbase-collator-node
        terminationGracePeriodSeconds: 60
        volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: chain-keystore
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: relaychain-keystore
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: chain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
        storageClassName: premium-rwo
        volumeMode: Filesystem
      status:
        phase: Pending
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: relaychain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: premium-rwo
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: localrococo-moonbase-collator-node-869bb577cf
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updateRevision: localrococo-moonbase-collator-node-869bb577cf
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: localrococo-statemint-alice
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    generation: 1
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-statemint-alice
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: statemint-local
      collatorAccount: 15oF4uVJwmo4TdGW7VfQxNLavjCXviqxT9S1MgbjMNHr6Sp5
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "1000"
      release: localrococo-statemint-alice
      role: collator
      ss58Format: "0"
    name: localrococo-statemint-alice-node
    namespace: rococo
    resourceVersion: "37021387"
    uid: c97a1174-9d3b-4225-bca5-a86f5cfdc8b9
  spec:
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: substrate-node
        app.kubernetes.io/instance: localrococo-statemint-alice
        app.kubernetes.io/name: node
    serviceName: localrococo-statemint-alice-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: substrate-node
          app.kubernetes.io/instance: localrococo-statemint-alice
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: node
          app.kubernetes.io/version: dev
          chain: statemint-local
          collatorAccount: 15oF4uVJwmo4TdGW7VfQxNLavjCXviqxT9S1MgbjMNHr6Sp5
          database: rocksdb
          helm.sh/chart: node-4.6.1
          paraId: "1000"
          release: localrococo-statemint-alice
          role: collator
          ss58Format: "0"
      spec:
        containers:
        - args:
          - -c
          - |
            set -eu
            POD_INDEX="${HOSTNAME##*-}"
            RELAY_CHAIN_P2P_PORT="30333"
            echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
            PARA_CHAIN_P2P_PORT="30334"
            echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
            exec /usr/local/bin/polkadot-parachain \
              --name=${POD_NAME} \
              --base-path=/chain-data \
              --keystore-path=/keystore \
              --chain=/chain-data/chainspec.json \
              --database=rocksdb \
              --collator \
              --prometheus-external \
              --prometheus-port 9615 \
              --unsafe-rpc-external \
              --unsafe-ws-external \
              --rpc-cors=all \
              --rpc-methods=unsafe \
              --listen-addr=/ip4/0.0.0.0/tcp/30334 \
              --node-key $(cat /custom-node-key/custom-node-key) \
              --alice \
              --ws-max-connections=5000 \
              -- \
              --name=${POD_NAME} \
              --base-path=/relaychain-data \
              --keystore-path=/relaychain-keystore \
              --database=rocksdb \
              --chain=/relaychain-data/relay_chain_chainspec.json \
              --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
              --listen-addr=/ip4/0.0.0.0/tcp/30333 \
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: statemint-local
          - name: NODE_NAME
            value: $(POD_NAME)
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ddorgan/polkadot-parachain:dev
          imagePullPolicy: Always
          name: statemint-local
          ports:
          - containerPort: 9933
            name: http-rpc
            protocol: TCP
          - containerPort: 9944
            name: websocket-rpc
            protocol: TCP
          - containerPort: 9615
            name: prometheus
            protocol: TCP
          - containerPort: 30333
            name: p2p
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http-rpc
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /keystore
            name: chain-keystore
          - mountPath: /relaychain-data
            name: relaychain-data
          - mountPath: /relaychain-keystore
            name: relaychain-keystore
          - mountPath: /custom-node-key/
            name: custom-node-key
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - |
            set -eu -o pipefail -x
              wget -O /chain-data/chainspec.json http://chainspec.rococo/statemint.json
              wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
          command:
          - /bin/sh
          image: paritytech/lz4:latest
          imagePullPolicy: Always
          name: download-chainspec
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /relaychain-data
            name: relaychain-data
        - args:
          - -c
          - |
            set -eu -x
            POD_INDEX="${HOSTNAME##*-}"
          command:
          - /bin/sh
          image: paritytech/kubetools-kubectl:latest
          imagePullPolicy: Always
          name: retrieve-service-info
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: localrococo-statemint-alice-node
        serviceAccountName: localrococo-statemint-alice-node
        terminationGracePeriodSeconds: 60
        volumes:
        - name: custom-node-key
          secret:
            defaultMode: 420
            secretName: localrococo-statemint-alice-node-custom-node-key
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: chain-keystore
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: relaychain-keystore
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: chain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: relaychain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: localrococo-statemint-alice-node-6cf4d55d66
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: localrococo-statemint-alice-node-6cf4d55d66
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: localrococo-statemint-bob
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T20:06:32Z"
    generation: 1
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-statemint-bob
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: statemint-local
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "1000"
      release: localrococo-statemint-bob
      role: collator
      ss58Format: "0"
    name: localrococo-statemint-bob-node
    namespace: rococo
    resourceVersion: "37024298"
    uid: 9f540e44-c36f-4917-b768-41f3c9244827
  spec:
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: substrate-node
        app.kubernetes.io/instance: localrococo-statemint-bob
        app.kubernetes.io/name: node
    serviceName: localrococo-statemint-bob-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: substrate-node
          app.kubernetes.io/instance: localrococo-statemint-bob
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: node
          app.kubernetes.io/version: dev
          chain: statemint-local
          database: rocksdb
          helm.sh/chart: node-4.6.1
          paraId: "1000"
          release: localrococo-statemint-bob
          role: collator
          ss58Format: "0"
      spec:
        containers:
        - args:
          - -c
          - |
            set -eu
            POD_INDEX="${HOSTNAME##*-}"
            RELAY_CHAIN_P2P_PORT="30333"
            echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
            PARA_CHAIN_P2P_PORT="30334"
            echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
            exec /usr/local/bin/polkadot-parachain \
              --name=${POD_NAME} \
              --base-path=/chain-data \
              --keystore-path=/keystore \
              --chain=/chain-data/chainspec.json \
              --database=rocksdb \
              --collator \
              --prometheus-external \
              --prometheus-port 9615 \
              --unsafe-rpc-external \
              --unsafe-ws-external \
              --rpc-cors=all \
              --rpc-methods=unsafe \
              --listen-addr=/ip4/0.0.0.0/tcp/30334 \
              --bob \
              --bootnodes  /dns4/localrococo-statemint-alice-node-0/tcp/30334/p2p/12D3KooWJsfMoQmEgsWK99oxUbFYFpwXnupQMJCDWW9AqARQ1CqM \
              -- \
              --name=${POD_NAME} \
              --base-path=/relaychain-data \
              --keystore-path=/relaychain-keystore \
              --database=rocksdb \
              --chain=/relaychain-data/relay_chain_chainspec.json \
              --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
              --listen-addr=/ip4/0.0.0.0/tcp/30333 \
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: statemint-local
          - name: NODE_NAME
            value: $(POD_NAME)
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ddorgan/polkadot-parachain:dev
          imagePullPolicy: Always
          name: statemint-local
          ports:
          - containerPort: 9933
            name: http-rpc
            protocol: TCP
          - containerPort: 9944
            name: websocket-rpc
            protocol: TCP
          - containerPort: 9615
            name: prometheus
            protocol: TCP
          - containerPort: 30333
            name: p2p
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http-rpc
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /keystore
            name: chain-keystore
          - mountPath: /relaychain-data
            name: relaychain-data
          - mountPath: /relaychain-keystore
            name: relaychain-keystore
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - |
            set -eu -o pipefail -x
              wget -O /chain-data/chainspec.json http://chainspec.rococo/statemint.json
              wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
          command:
          - /bin/sh
          image: paritytech/lz4:latest
          imagePullPolicy: Always
          name: download-chainspec
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /relaychain-data
            name: relaychain-data
        - args:
          - -c
          - |
            set -eu -x
            POD_INDEX="${HOSTNAME##*-}"
          command:
          - /bin/sh
          image: paritytech/kubetools-kubectl:latest
          imagePullPolicy: Always
          name: retrieve-service-info
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: localrococo-statemint-bob-node
        serviceAccountName: localrococo-statemint-bob-node
        terminationGracePeriodSeconds: 60
        volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: chain-keystore
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: relaychain-keystore
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: chain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: relaychain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 1
    currentRevision: localrococo-statemint-bob-node-79bf47c79c
    observedGeneration: 1
    replicas: 1
    updateRevision: localrococo-statemint-bob-node-79bf47c79c
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: localrococo-tellor-alice
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T22:03:40Z"
    generation: 1
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-tellor-alice
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: rust-locked
      chain: polkadot
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "3000"
      release: localrococo-tellor-alice
      role: collator
    name: localrococo-tellor-alice-node
    namespace: rococo
    resourceVersion: "36466785"
    uid: b04b97ce-43a0-458e-938b-e1a04ff976f0
  spec:
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: substrate-node
        app.kubernetes.io/instance: localrococo-tellor-alice
        app.kubernetes.io/name: node
    serviceName: localrococo-tellor-alice-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: substrate-node
          app.kubernetes.io/instance: localrococo-tellor-alice
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: node
          app.kubernetes.io/version: rust-locked
          chain: polkadot
          database: rocksdb
          helm.sh/chart: node-4.6.1
          paraId: "3000"
          release: localrococo-tellor-alice
          role: collator
      spec:
        containers:
        - args:
          - -c
          - |
            set -eu
            POD_INDEX="${HOSTNAME##*-}"
            RELAY_CHAIN_P2P_PORT="30333"
            echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
            PARA_CHAIN_P2P_PORT="30334"
            echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
            exec /usr/local/bin/parachain-template-node \
              --name=${POD_NAME} \
              --base-path=/chain-data \
              --keystore-path=/keystore \
              --chain=/chain-data/chainspec.json \
              --database=rocksdb \
              --collator \
              --prometheus-external \
              --prometheus-port 9615 \
              --unsafe-rpc-external \
              --unsafe-ws-external \
              --rpc-cors=all \
              --rpc-methods=unsafe \
              --listen-addr=/ip4/0.0.0.0/tcp/30334 \
              --node-key $(cat /custom-node-key/custom-node-key) \
              --alice \
              --ws-max-connections=5000 \
              -- \
              --name=${POD_NAME} \
              --base-path=/relaychain-data \
              --keystore-path=/relaychain-keystore \
              --database=rocksdb \
              --chain=/relaychain-data/relay_chain_chainspec.json \
              --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
              --listen-addr=/ip4/0.0.0.0/tcp/30333 \
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: polkadot
          - name: NODE_NAME
            value: $(POD_NAME)
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ddorgan/tellor-oracle:rust-locked
          imagePullPolicy: Always
          name: polkadot
          ports:
          - containerPort: 9933
            name: http-rpc
            protocol: TCP
          - containerPort: 9944
            name: websocket-rpc
            protocol: TCP
          - containerPort: 9615
            name: prometheus
            protocol: TCP
          - containerPort: 30333
            name: p2p
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http-rpc
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /keystore
            name: chain-keystore
          - mountPath: /relaychain-data
            name: relaychain-data
          - mountPath: /relaychain-keystore
            name: relaychain-keystore
          - mountPath: /custom-node-key/
            name: custom-node-key
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - |
            set -eu -o pipefail -x
              wget -O /chain-data/chainspec.json http://chainspec.rococo/tellor.json
              wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
          command:
          - /bin/sh
          image: paritytech/lz4:latest
          imagePullPolicy: Always
          name: download-chainspec
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /relaychain-data
            name: relaychain-data
        - args:
          - -c
          - |
            set -eu -x
            POD_INDEX="${HOSTNAME##*-}"
          command:
          - /bin/sh
          image: paritytech/kubetools-kubectl:latest
          imagePullPolicy: Always
          name: retrieve-service-info
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: localrococo-tellor-alice-node
        serviceAccountName: localrococo-tellor-alice-node
        terminationGracePeriodSeconds: 60
        volumes:
        - name: custom-node-key
          secret:
            defaultMode: 420
            secretName: localrococo-tellor-alice-node-custom-node-key
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: chain-keystore
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: relaychain-keystore
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: chain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: relaychain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 1
    currentRevision: localrococo-tellor-alice-node-5c5cd664f5
    observedGeneration: 1
    replicas: 1
    updateRevision: localrococo-tellor-alice-node-5c5cd664f5
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: localrococo-tellor-bob
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-16T22:03:39Z"
    generation: 1
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-tellor-bob
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: rust-locked
      chain: polkadot
      database: rocksdb
      helm.sh/chart: node-4.6.1
      paraId: "3000"
      release: localrococo-tellor-bob
      role: collator
    name: localrococo-tellor-bob-node
    namespace: rococo
    resourceVersion: "36466799"
    uid: f1c976cf-1a5e-458b-8b16-6ea37b133d9c
  spec:
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: substrate-node
        app.kubernetes.io/instance: localrococo-tellor-bob
        app.kubernetes.io/name: node
    serviceName: localrococo-tellor-bob-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: substrate-node
          app.kubernetes.io/instance: localrococo-tellor-bob
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: node
          app.kubernetes.io/version: rust-locked
          chain: polkadot
          database: rocksdb
          helm.sh/chart: node-4.6.1
          paraId: "3000"
          release: localrococo-tellor-bob
          role: collator
      spec:
        containers:
        - args:
          - -c
          - |
            set -eu
            POD_INDEX="${HOSTNAME##*-}"
            RELAY_CHAIN_P2P_PORT="30333"
            echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
            PARA_CHAIN_P2P_PORT="30334"
            echo "PARA_CHAIN_P2P_PORT=${PARA_CHAIN_P2P_PORT}"
            exec /usr/local/bin/parachain-template-node \
              --name=${POD_NAME} \
              --base-path=/chain-data \
              --keystore-path=/keystore \
              --chain=/chain-data/chainspec.json \
              --database=rocksdb \
              --collator \
              --prometheus-external \
              --prometheus-port 9615 \
              --unsafe-rpc-external \
              --unsafe-ws-external \
              --rpc-cors=all \
              --rpc-methods=unsafe \
              --listen-addr=/ip4/0.0.0.0/tcp/30334 \
              --bob \
              --bootnodes  /dns4/localrococo-tellor-alice-node-0/tcp/30334/p2p/12D3KooWRBfVfhHEjbLvrFU9koaWwxZHvtaiMkyYnwHnfwoiTwoY \
              -- \
              --name=${POD_NAME} \
              --base-path=/relaychain-data \
              --keystore-path=/relaychain-keystore \
              --database=rocksdb \
              --chain=/relaychain-data/relay_chain_chainspec.json \
              --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
              --listen-addr=/ip4/0.0.0.0/tcp/30333 \
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: polkadot
          - name: NODE_NAME
            value: $(POD_NAME)
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ddorgan/tellor-oracle:rust-locked
          imagePullPolicy: Always
          name: polkadot
          ports:
          - containerPort: 9933
            name: http-rpc
            protocol: TCP
          - containerPort: 9944
            name: websocket-rpc
            protocol: TCP
          - containerPort: 9615
            name: prometheus
            protocol: TCP
          - containerPort: 30333
            name: p2p
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http-rpc
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /keystore
            name: chain-keystore
          - mountPath: /relaychain-data
            name: relaychain-data
          - mountPath: /relaychain-keystore
            name: relaychain-keystore
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - |
            set -eu -o pipefail -x
              wget -O /chain-data/chainspec.json http://chainspec.rococo/tellor.json
              wget -O /relaychain-data/relay_chain_chainspec.json http://chainspec.rococo/rococo-local.json
          command:
          - /bin/sh
          image: paritytech/lz4:latest
          imagePullPolicy: Always
          name: download-chainspec
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /relaychain-data
            name: relaychain-data
        - args:
          - -c
          - |
            set -eu -x
            POD_INDEX="${HOSTNAME##*-}"
          command:
          - /bin/sh
          image: paritytech/kubetools-kubectl:latest
          imagePullPolicy: Always
          name: retrieve-service-info
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: localrococo-tellor-bob-node
        serviceAccountName: localrococo-tellor-bob-node
        terminationGracePeriodSeconds: 60
        volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: chain-keystore
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: relaychain-keystore
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: chain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: relaychain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        storageClassName: standard
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 1
    currentRevision: localrococo-tellor-bob-node-577454f9b8
    observedGeneration: 1
    replicas: 1
    updateRevision: localrococo-tellor-bob-node-577454f9b8
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: localrococo-validator-a
      meta.helm.sh/release-namespace: rococo
    creationTimestamp: "2023-05-19T02:57:47Z"
    generation: 1
    labels:
      app.kubernetes.io/component: substrate-node
      app.kubernetes.io/instance: localrococo-validator-a
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node
      app.kubernetes.io/version: dev
      chain: rococo-local
      database: rocksdb
      helm.sh/chart: node-4.6.1
      pruning: archive
      release: localrococo-validator-a
      role: authority
    name: localrococo-validator-a-node
    namespace: rococo
    resourceVersion: "36992700"
    uid: 54e73c5a-6d46-4e34-ade7-dad1c2a50e09
  spec:
    podManagementPolicy: Parallel
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: substrate-node
        app.kubernetes.io/instance: localrococo-validator-a
        app.kubernetes.io/name: node
    serviceName: localrococo-validator-a-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: substrate-node
          app.kubernetes.io/instance: localrococo-validator-a
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: node
          app.kubernetes.io/version: dev
          chain: rococo-local
          database: rocksdb
          helm.sh/chart: node-4.6.1
          pruning: archive
          release: localrococo-validator-a
          role: authority
      spec:
        containers:
        - args:
          - -c
          - |
            set -eu
            POD_INDEX="${HOSTNAME##*-}"
            RELAY_CHAIN_P2P_PORT="30333"
            echo "RELAY_CHAIN_P2P_PORT=${RELAY_CHAIN_P2P_PORT}"
            exec polkadot \
              --name=${POD_NAME} \
              --base-path=/chain-data \
              --keystore-path=/keystore \
              --chain=/chain-data/chainspec.json \
              --validator \
              --database=rocksdb \
              --pruning=archive \
              --prometheus-external \
              --prometheus-port 9615 \
              --unsafe-rpc-external \
              --unsafe-ws-external \
              --rpc-cors=all \
              --rpc-methods=unsafe \
              --bootnodes  /dns4/localrococo-bootnode-0/tcp/30333/p2p/12D3KooWRrehp4u6ZmqeMPJ75kJD1S8b9aqfQ3qcaU4TWthpYnoL \
              --listen-addr=/ip4/0.0.0.0/tcp/30333 \
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: rococo-local
          - name: NODE_NAME
            value: $(POD_NAME)
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ddorgan/tellor-polkadot:dev
          imagePullPolicy: Always
          name: rococo-local
          ports:
          - containerPort: 9933
            name: http-rpc
            protocol: TCP
          - containerPort: 9944
            name: websocket-rpc
            protocol: TCP
          - containerPort: 9615
            name: prometheus
            protocol: TCP
          - containerPort: 30333
            name: p2p
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http-rpc
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
          - mountPath: /keystore
            name: chain-keystore
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -c
          - |
            set -eu -o pipefail -x
              wget -O /chain-data/chainspec.json http://chainspec.rococo/rococo-local.json
          command:
          - /bin/sh
          image: paritytech/lz4:latest
          imagePullPolicy: Always
          name: download-chainspec
          resources: {}
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        - args:
          - -c
          - |
            set -eu -x
            if [ ! -f /var/run/secrets/gran/type ]; then
               echo "Error: File /var/run/secrets/gran/type does not exist"
               exit 1
            fi
            polkadot key insert \
            --keystore-path /keystore \
            --key-type $(cat /var/run/secrets/gran/type) \
            --scheme $(cat /var/run/secrets/gran/scheme) \
            --suri "$(cat /var/run/secrets/gran/seed)//validator//${HOSTNAME}" \
            && echo "Inserted key gran into Keystore" \
            || echo "Failed to insert key gran into Keystore."
            if [ ! -f /var/run/secrets/babe/type ]; then
               echo "Error: File /var/run/secrets/babe/type does not exist"
               exit 1
            fi
            polkadot key insert \
            --keystore-path /keystore \
            --key-type $(cat /var/run/secrets/babe/type) \
            --scheme $(cat /var/run/secrets/babe/scheme) \
            --suri "$(cat /var/run/secrets/babe/seed)//validator//${HOSTNAME}" \
            && echo "Inserted key babe into Keystore" \
            || echo "Failed to insert key babe into Keystore."
          command:
          - /bin/sh
          env:
          - name: CHAIN
            value: rococo-local
          image: ddorgan/tellor-polkadot:dev
          imagePullPolicy: IfNotPresent
          name: inject-keys
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /keystore
            name: chain-keystore
          - mountPath: /var/run/secrets/gran
            name: gran
          - mountPath: /var/run/secrets/babe
            name: babe
        - args:
          - -c
          - |
            set -eu -x
            POD_INDEX="${HOSTNAME##*-}"
          command:
          - /bin/sh
          image: paritytech/kubetools-kubectl:latest
          imagePullPolicy: Always
          name: retrieve-service-info
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /chain-data
            name: chain-data
        - args:
          - /keystore
          image: docker.io/paritytech/substrate-session-keys-grabber:d17032f1-20221202
          imagePullPolicy: IfNotPresent
          name: dump-session-keys
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /keystore
            name: chain-keystore
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: localrococo-validator-a-node
        serviceAccountName: localrococo-validator-a-node
        terminationGracePeriodSeconds: 60
        volumes:
        - name: gran
          secret:
            defaultMode: 256
            secretName: localrococo-validator-a-node-gran
        - name: babe
          secret:
            defaultMode: 256
            secretName: localrococo-validator-a-node-babe
        - emptyDir:
            medium: Memory
            sizeLimit: 10Mi
          name: chain-keystore
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: chain-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi
        storageClassName: premium-rwo
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 2
    collisionCount: 0
    currentReplicas: 2
    currentRevision: localrococo-validator-a-node-6cd9d86dd5
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updateRevision: localrococo-validator-a-node-6cd9d86dd5
    updatedReplicas: 2
kind: List
metadata:
  resourceVersion: ""
